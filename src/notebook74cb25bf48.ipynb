{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b653ffed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T15:33:03.384416Z",
     "iopub.status.busy": "2023-05-01T15:33:03.383261Z",
     "iopub.status.idle": "2023-05-01T15:33:03.392877Z",
     "shell.execute_reply": "2023-05-01T15:33:03.391509Z",
     "shell.execute_reply.started": "2023-05-01T15:33:03.384322Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 2 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a6b88a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T15:33:03.400182Z",
     "iopub.status.busy": "2023-05-01T15:33:03.399911Z",
     "iopub.status.idle": "2023-05-01T15:33:03.408797Z",
     "shell.execute_reply": "2023-05-01T15:33:03.407578Z",
     "shell.execute_reply.started": "2023-05-01T15:33:03.400134Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\data\\\\images\\\\data'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 8\n",
    "ARCH = \"flor\"\n",
    "\n",
    "IMG_SIZE = (128,32, 1)\n",
    "DATA_ROOT_PATH = \"..\\\\data\"\n",
    "IMAGES_PATH = os.path.join(DATA_ROOT_PATH, \"images\", \"data\")\n",
    "IMAGES_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "902c05c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T15:33:03.412476Z",
     "iopub.status.busy": "2023-05-01T15:33:03.412107Z",
     "iopub.status.idle": "2023-05-01T15:33:03.430547Z",
     "shell.execute_reply": "2023-05-01T15:33:03.429572Z",
     "shell.execute_reply.started": "2023-05-01T15:33:03.412439Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = None\n",
    "\n",
    "with open(os.path.join(DATA_ROOT_PATH, \"hindi_vocab.txt\"),encoding=\"utf-8\") as f:\n",
    "  vocab = f.readlines()\n",
    "\n",
    "idx_to_vocab = {i:value.strip() for i, value in enumerate(vocab)}\n",
    "vocab_to_idx = {value:key for key, value in idx_to_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "843eb569",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T15:33:03.432293Z",
     "iopub.status.busy": "2023-05-01T15:33:03.432016Z",
     "iopub.status.idle": "2023-05-01T15:33:03.448369Z",
     "shell.execute_reply": "2023-05-01T15:33:03.447223Z",
     "shell.execute_reply.started": "2023-05-01T15:33:03.432257Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "train_data = None\n",
    "\n",
    "with open(os.path.join(DATA_ROOT_PATH, \"new_train.txt\"), encoding=\"utf-8\") as f:\n",
    "  train_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e67ffba1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T15:33:03.450224Z",
     "iopub.status.busy": "2023-05-01T15:33:03.449883Z",
     "iopub.status.idle": "2023-05-01T15:33:03.458870Z",
     "shell.execute_reply": "2023-05-01T15:33:03.457832Z",
     "shell.execute_reply.started": "2023-05-01T15:33:03.450150Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_data = None\n",
    "\n",
    "with open(os.path.join(DATA_ROOT_PATH, \"new_val.txt\"), encoding=\"utf-8\") as f:\n",
    "  valid_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de77b731",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T15:33:03.462873Z",
     "iopub.status.busy": "2023-05-01T15:33:03.462668Z",
     "iopub.status.idle": "2023-05-01T15:33:03.471700Z",
     "shell.execute_reply": "2023-05-01T15:33:03.470708Z",
     "shell.execute_reply.started": "2023-05-01T15:33:03.462846Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "test_data = None\n",
    "\n",
    "with open(os.path.join(DATA_ROOT_PATH, \"new_test.txt\"), encoding=\"utf-8\") as f:\n",
    "  test_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ce600af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T15:33:03.474105Z",
     "iopub.status.busy": "2023-05-01T15:33:03.473109Z",
     "iopub.status.idle": "2023-05-01T15:33:03.482196Z",
     "shell.execute_reply": "2023-05-01T15:33:03.481089Z",
     "shell.execute_reply.started": "2023-05-01T15:33:03.474067Z"
    }
   },
   "outputs": [],
   "source": [
    "charl = None\n",
    "\n",
    "with open( \"charList.txt\", encoding=\"utf-8\") as f:\n",
    "  charl = f.readlines()\n",
    "charl = charl[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e613d291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f625e08a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e30859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_data = train.split()\n",
    "curr_img_path = curr_data[0]\n",
    "\n",
    "curr_label = label_g(int(curr_data[1]))\n",
    "\n",
    "curr_img_path = \"/\".join(curr_img_path.split(\"/\")[2:])\n",
    "curr_img_path = os.path.join(IMAGES_PATH, curr_img_path)\n",
    "\n",
    "curr_img = preprocess(curr_img_path, self.img_size)\n",
    "curr_img = cv2.adaptiveThreshold(curr_img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 199, 5)\n",
    "curr_img = curr_img[:, :, np.newaxis]  # add new axis for channels\n",
    "\n",
    "# Apply data augmentation\n",
    "curr_img = self.datagen.random_transform(curr_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6dcae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32d98941",
   "metadata": {},
   "source": [
    "# train data labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3140313",
   "metadata": {},
   "source": [
    "# train data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "707d127a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T15:33:03.484552Z",
     "iopub.status.busy": "2023-05-01T15:33:03.484218Z",
     "iopub.status.idle": "2023-05-01T15:33:03.490534Z",
     "shell.execute_reply": "2023-05-01T15:33:03.489406Z",
     "shell.execute_reply.started": "2023-05-01T15:33:03.484514Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import tensorflow as tf\n",
    "from PIL import Image, ImageOps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec092925",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T15:33:03.492983Z",
     "iopub.status.busy": "2023-05-01T15:33:03.491958Z",
     "iopub.status.idle": "2023-05-01T15:33:03.499448Z",
     "shell.execute_reply": "2023-05-01T15:33:03.498609Z",
     "shell.execute_reply.started": "2023-05-01T15:33:03.492945Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "##label generator\n",
    "def label_g(l):\n",
    "    label=idx_to_vocab[l]\n",
    "    z=[]\n",
    "    for j in range(27):\n",
    "        z.append(0)\n",
    "    for k in range(len(label)):\n",
    "        for r in range(len(charl)):\n",
    "            if label[k]==charl[r]:\n",
    "                z[k]=r\n",
    "    return z   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c6ebfcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T15:33:03.501659Z",
     "iopub.status.busy": "2023-05-01T15:33:03.500793Z",
     "iopub.status.idle": "2023-05-01T15:33:03.508625Z",
     "shell.execute_reply": "2023-05-01T15:33:03.507653Z",
     "shell.execute_reply.started": "2023-05-01T15:33:03.501620Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "##label generator\n",
    "def label_g1(l):\n",
    "    label=idx_to_vocab[l]\n",
    "    z=[]\n",
    "    for k in range(len(label)):\n",
    "        for r in range(len(charl)):\n",
    "            if label[k]==charl[r]:\n",
    "                z.append(r)\n",
    "                \n",
    "    return z   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7fc77a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T15:33:03.568050Z",
     "iopub.status.busy": "2023-05-01T15:33:03.567853Z",
     "iopub.status.idle": "2023-05-01T15:33:03.605921Z",
     "shell.execute_reply": "2023-05-01T15:33:03.604851Z",
     "shell.execute_reply.started": "2023-05-01T15:33:03.568024Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import cv2\n",
    "import html\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def adjust_to_see(img):\n",
    "    \"\"\"Rotate and transpose to image visualize (cv2 method or jupyter notebook)\"\"\"\n",
    "\n",
    "    (h, w) = img.shape[:2]\n",
    "    (cX, cY) = (w // 2, h // 2)\n",
    "\n",
    "    M = cv2.getRotationMatrix2D((cX, cY), -90, 1.0)\n",
    "    cos = np.abs(M[0, 0])\n",
    "    sin = np.abs(M[0, 1])\n",
    "\n",
    "    nW = int((h * sin) + (w * cos))\n",
    "    nH = int((h * cos) + (w * sin))\n",
    "\n",
    "    M[0, 2] += (nW / 2) - cX\n",
    "    M[1, 2] += (nH / 2) - cY\n",
    "\n",
    "    img = cv2.warpAffine(img, M, (nW + 1, nH + 1))\n",
    "    img = cv2.warpAffine(img.transpose(), M, (nW, nH))\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def augmentation(imgs,\n",
    "                 rotation_range=0,\n",
    "                 scale_range=0,\n",
    "                 height_shift_range=0,\n",
    "                 width_shift_range=0,\n",
    "                 dilate_range=1,\n",
    "                 erode_range=1):\n",
    "    \"\"\"Apply variations to a list of images (rotate, width and height shift, scale, erode, dilate)\"\"\"\n",
    "\n",
    "    imgs = imgs.astype(np.float32)\n",
    "    _, h, w = imgs.shape\n",
    "\n",
    "    dilate_kernel = np.ones((int(np.random.uniform(1, dilate_range)),), np.uint8)\n",
    "    erode_kernel = np.ones((int(np.random.uniform(1, erode_range)),), np.uint8)\n",
    "    height_shift = np.random.uniform(-height_shift_range, height_shift_range)\n",
    "    rotation = np.random.uniform(-rotation_range, rotation_range)\n",
    "    scale = np.random.uniform(1 - scale_range, 1)\n",
    "    width_shift = np.random.uniform(-width_shift_range, width_shift_range)\n",
    "\n",
    "    trans_map = np.float32([[1, 0, width_shift * w], [0, 1, height_shift * h]])\n",
    "    rot_map = cv2.getRotationMatrix2D((w // 2, h // 2), rotation, scale)\n",
    "\n",
    "    trans_map_aff = np.r_[trans_map, [[0, 0, 1]]]\n",
    "    rot_map_aff = np.r_[rot_map, [[0, 0, 1]]]\n",
    "    affine_mat = rot_map_aff.dot(trans_map_aff)[:2, :]\n",
    "\n",
    "    for i in range(len(imgs)):\n",
    "        imgs[i] = cv2.warpAffine(imgs[i], affine_mat, (w, h), flags=cv2.INTER_NEAREST, borderValue=255)\n",
    "        imgs[i] = cv2.erode(imgs[i], erode_kernel, iterations=1)\n",
    "        imgs[i] = cv2.dilate(imgs[i], dilate_kernel, iterations=1)\n",
    "\n",
    "    return imgs\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DeepSpell based text cleaning process.\n",
    "    Tal Weiss.\n",
    "    Deep Spelling.\n",
    "    Medium: https://machinelearnings.co/deep-spelling-9ffef96a24f6#.2c9pu8nlm\n",
    "    Github: https://github.com/MajorTal/DeepSpell\n",
    "\"\"\"\n",
    "\n",
    "RE_DASH_FILTER = re.compile(r'[\\-\\˗\\֊\\‐\\‑\\‒\\–\\—\\⁻\\₋\\−\\﹣\\－]', re.UNICODE)\n",
    "RE_APOSTROPHE_FILTER = re.compile(r'&#39;|[ʼ՚＇‘’‛❛❜ߴߵ`‵´ˊˋ{}{}{}{}{}{}{}{}{}]'.format(\n",
    "    chr(768), chr(769), chr(832), chr(833), chr(2387),\n",
    "    chr(5151), chr(5152), chr(65344), chr(8242)), re.UNICODE)\n",
    "RE_RESERVED_CHAR_FILTER = re.compile(r'[¶¤«»]', re.UNICODE)\n",
    "RE_LEFT_PARENTH_FILTER = re.compile(r'[\\(\\[\\{\\⁽\\₍\\❨\\❪\\﹙\\（]', re.UNICODE)\n",
    "RE_RIGHT_PARENTH_FILTER = re.compile(r'[\\)\\]\\}\\⁾\\₎\\❩\\❫\\﹚\\）]', re.UNICODE)\n",
    "RE_BASIC_CLEANER = re.compile(r'[^\\w\\s{}]'.format(re.escape(string.punctuation)), re.UNICODE)\n",
    "\n",
    "LEFT_PUNCTUATION_FILTER = \"\"\"!%&),.:;<=>?@\\\\]^_`|}~\"\"\"\n",
    "RIGHT_PUNCTUATION_FILTER = \"\"\"\"(/<=>@[\\\\^_`{|~\"\"\"\n",
    "NORMALIZE_WHITESPACE_REGEX = re.compile(r'[^\\S\\n]+', re.UNICODE)\n",
    "\n",
    "\n",
    "def text_standardize(text):\n",
    "    \"\"\"Organize/add spaces around punctuation marks\"\"\"\n",
    "\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "\n",
    "    text = html.unescape(text).replace(\"\\\\n\", \"\").replace(\"\\\\t\", \"\")\n",
    "\n",
    "    text = RE_RESERVED_CHAR_FILTER.sub(\"\", text)\n",
    "    text = RE_DASH_FILTER.sub(\"-\", text)\n",
    "    text = RE_APOSTROPHE_FILTER.sub(\"'\", text)\n",
    "    text = RE_LEFT_PARENTH_FILTER.sub(\"(\", text)\n",
    "    text = RE_RIGHT_PARENTH_FILTER.sub(\")\", text)\n",
    "    text = RE_BASIC_CLEANER.sub(\"\", text)\n",
    "\n",
    "    text = text.lstrip(LEFT_PUNCTUATION_FILTER)\n",
    "    text = text.rstrip(RIGHT_PUNCTUATION_FILTER)\n",
    "    text = text.translate(str.maketrans({c: f\" {c} \" for c in string.punctuation}))\n",
    "    text = NORMALIZE_WHITESPACE_REGEX.sub(\" \", text.strip())\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalization(imgs):\n",
    "    \"\"\"Normalize list of images\"\"\"\n",
    "\n",
    "    imgs = np.asarray(imgs).astype(np.float32)\n",
    "    _, h, w = imgs.shape\n",
    "\n",
    "    for i in range(len(imgs)):\n",
    "        m, s = cv2.meanStdDev(imgs[i])\n",
    "        imgs[i] = imgs[i] - m[0][0]\n",
    "        imgs[i] = imgs[i] / s[0][0] if s[0][0] > 0 else imgs[i]\n",
    "\n",
    "    return np.expand_dims(imgs, axis=-1)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Preprocess metodology based in:\n",
    "    H. Scheidl, S. Fiel and R. Sablatnig,\n",
    "    Word Beam Search: A Connectionist Temporal Classification Decoding Algorithm, in\n",
    "    16th International Conference on Frontiers in Handwriting Recognition, pp. 256-258, 2018.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def preprocess(img, input_size):\n",
    "    \"\"\"Make the process with the `input_size` to the scale resize\"\"\"\n",
    "\n",
    "    def imread(path):\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        u, i = np.unique(np.array(img).flatten(), return_inverse=True)\n",
    "        background = int(u[np.argmax(np.bincount(i))])\n",
    "        # plt.imshow(img)\n",
    "        return img, background\n",
    "\n",
    "    if isinstance(img, str):\n",
    "        img, bg = imread(img)\n",
    "\n",
    "    if isinstance(img, tuple):\n",
    "        image, boundbox = img\n",
    "        img, bg = imread(image)\n",
    "\n",
    "        for i in range(len(boundbox)):\n",
    "            if isinstance(boundbox[i], float):\n",
    "                total = len(img) if i < 2 else len(img[0])\n",
    "                boundbox[i] = int(total * boundbox[i])\n",
    "            else:\n",
    "                boundbox[i] = int(boundbox[i])\n",
    "\n",
    "        img = np.asarray(img[boundbox[0]:boundbox[1], boundbox[2]:boundbox[3]], dtype=np.uint8)\n",
    "\n",
    "    wt, ht = input_size[:-1]\n",
    "    h, w = np.asarray(img).shape\n",
    "    f = max((h / ht), (w / wt) )\n",
    "\n",
    "    new_size = (max(min(wt, int(w / f)), 1), max(min(ht, int(h / f)), 1))\n",
    "\n",
    "    img = cv2.resize(img, new_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    target = np.ones([ht, wt], dtype=np.uint8) * bg\n",
    "    target[0:new_size[1], 0:new_size[0]] = img\n",
    "    img = target\n",
    "    # plt.imshow(img)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86cf7808",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T15:33:03.609755Z",
     "iopub.status.busy": "2023-05-01T15:33:03.608993Z",
     "iopub.status.idle": "2023-05-01T15:33:03.623621Z",
     "shell.execute_reply": "2023-05-01T15:33:03.622305Z",
     "shell.execute_reply.started": "2023-05-01T15:33:03.609711Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=np.zeros((8, 27))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aaaee328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as im\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "306900e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T15:33:03.625908Z",
     "iopub.status.busy": "2023-05-01T15:33:03.625467Z",
     "iopub.status.idle": "2023-05-01T15:33:03.638325Z",
     "shell.execute_reply": "2023-05-01T15:33:03.637405Z",
     "shell.execute_reply.started": "2023-05-01T15:33:03.625871Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataGen(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data, img_size, batch_size, mode=\"TRAIN\"):\n",
    "        self.data = data\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mode = mode\n",
    "        self.datagen = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=5, width_shift_range=0.05,\n",
    "                                                                        height_shift_range=0.05, shear_range=0.05,\n",
    "                                                                        zoom_range=0.05)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        start = i * self.batch_size\n",
    "        end = (i+1) * self.batch_size\n",
    "        batch_images = np.zeros((self.batch_size, self.img_size[0], self.img_size[1], 1))\n",
    "        batch_labels = np.zeros((self.batch_size, 27))\n",
    "\n",
    "        for ii, df_index in enumerate(range(start, end)):\n",
    "            curr_data = self.data[df_index].split()\n",
    "            curr_img_path = curr_data[0]\n",
    "          \n",
    "            curr_label = label_g(int(curr_data[1]))\n",
    "\n",
    "            curr_img_path = \"/\".join(curr_img_path.split(\"/\")[2:])\n",
    "            curr_img_path = os.path.join(IMAGES_PATH, curr_img_path)\n",
    "\n",
    "            curr_img = preprocess(curr_img_path, self.img_size)\n",
    "            curr_img = cv2.adaptiveThreshold(curr_img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 199, 5)\n",
    "            curr_img = curr_img[:, :, np.newaxis]  # add new axis for channels\n",
    "\n",
    "            # Apply data augmentation\n",
    "            curr_img = self.datagen.random_transform(curr_img)\n",
    "            data = im.fromarray(curr_img)\n",
    "            plt.imshow(data)\n",
    "\n",
    "            batch_images[ii, :, :, 0] = np.transpose(curr_img, (1, 0, 2))[:, :, 0]\n",
    "            batch_labels[ii,:] = curr_label\n",
    "                    \n",
    "        if self.mode == \"TRAIN\":\n",
    "          return batch_images, batch_labels\n",
    "        else:\n",
    "          return batch_images\n",
    "        \n",
    "    def __len__(self):\n",
    "      return len(self.data) // self.batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d5d0a84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T15:33:03.640385Z",
     "iopub.status.busy": "2023-05-01T15:33:03.640073Z",
     "iopub.status.idle": "2023-05-01T15:33:03.653208Z",
     "shell.execute_reply": "2023-05-01T15:33:03.652235Z",
     "shell.execute_reply.started": "2023-05-01T15:33:03.640350Z"
    }
   },
   "outputs": [],
   "source": [
    "train_datagen = DataGen(train_data, IMG_SIZE,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8cefa84",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 1), |u1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\pavan\\lib\\site-packages\\PIL\\Image.py:3080\u001b[0m, in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   3079\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3080\u001b[0m     mode, rawmode \u001b[38;5;241m=\u001b[39m \u001b[43m_fromarray_typemap\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtypekey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   3081\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyError\u001b[0m: ((1, 1, 1), '|u1')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_datagen\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 32\u001b[0m, in \u001b[0;36mDataGen.__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Apply data augmentation\u001b[39;00m\n\u001b[0;32m     31\u001b[0m curr_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatagen\u001b[38;5;241m.\u001b[39mrandom_transform(curr_img)\n\u001b[1;32m---> 32\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(data)\n\u001b[0;32m     35\u001b[0m batch_images[ii, :, :, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(curr_img, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m))[:, :, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pavan\\lib\\site-packages\\PIL\\Image.py:3083\u001b[0m, in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   3081\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   3082\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot handle this data type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m typekey\n\u001b[1;32m-> 3083\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   3084\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3085\u001b[0m     rawmode \u001b[38;5;241m=\u001b[39m mode\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 1), |u1"
     ]
    }
   ],
   "source": [
    "train_datagen[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1642ae38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T15:33:03.656486Z",
     "iopub.status.busy": "2023-05-01T15:33:03.656217Z",
     "iopub.status.idle": "2023-05-01T15:33:03.663139Z",
     "shell.execute_reply": "2023-05-01T15:33:03.662222Z",
     "shell.execute_reply.started": "2023-05-01T15:33:03.656443Z"
    }
   },
   "outputs": [],
   "source": [
    "test_datagen = DataGen(test_data, IMG_SIZE, 1000\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c50d1d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T15:33:03.665111Z",
     "iopub.status.busy": "2023-05-01T15:33:03.664848Z",
     "iopub.status.idle": "2023-05-01T15:33:03.672505Z",
     "shell.execute_reply": "2023-05-01T15:33:03.671523Z",
     "shell.execute_reply.started": "2023-05-01T15:33:03.665073Z"
    }
   },
   "outputs": [],
   "source": [
    "validation_datagen = DataGen(valid_data, IMG_SIZE, 32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2451e4b4",
   "metadata": {},
   "source": [
    "# model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61be3060",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T15:33:03.674909Z",
     "iopub.status.busy": "2023-05-01T15:33:03.674606Z",
     "iopub.status.idle": "2023-05-01T15:33:03.705748Z",
     "shell.execute_reply": "2023-05-01T15:33:03.704841Z",
     "shell.execute_reply.started": "2023-05-01T15:33:03.674874Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gated implementations\n",
    "    GatedConv2D: Introduce a Conv2D layer (same number of filters) to multiply with its sigmoid activation.\n",
    "    FullGatedConv2D: Introduce a Conv2D to extract features (linear and sigmoid), making a full gated process.\n",
    "                     This process will double number of filters to make one convolutional process.\n",
    "\"\"\"\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Multiply, Activation\n",
    "\n",
    "\"\"\"\n",
    "Tensorflow Keras layer implementation of the gated convolution.\n",
    "    Args:\n",
    "        kwargs: Conv2D keyword arguments.\n",
    "    Reference:\n",
    "        T. Bluche, R. Messina,\n",
    "        Gated convolutional recurrent neural networks for multilingual handwriting recognition.\n",
    "        14th IAPR International Conference on Document Analysis andRecognition (ICDAR),\n",
    "        p. 646–651, 11 2017.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class GatedConv2D(Conv2D):\n",
    "    \"\"\"Gated Convolutional Class\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(GatedConv2D, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Apply gated convolution\"\"\"\n",
    "\n",
    "        output = super(GatedConv2D, self).call(inputs)\n",
    "        linear = Activation(\"linear\")(inputs)\n",
    "        sigmoid = Activation(\"sigmoid\")(output)\n",
    "\n",
    "        return Multiply()([linear, sigmoid])\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"Return the config of the layer\"\"\"\n",
    "\n",
    "        config = super(GatedConv2D, self).get_config()\n",
    "        return config\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Tensorflow Keras layer implementation of the gated convolution.\n",
    "    Args:\n",
    "        filters (int): Number of output filters.\n",
    "        kwargs: Other Conv2D keyword arguments.\n",
    "    Reference (based):\n",
    "        Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier,\n",
    "        Language modeling with gated convolutional networks, in\n",
    "        Proc. 34th Int. Conf. Mach. Learn. (ICML), vol. 70,\n",
    "        Sydney, Australia, pp. 933–941, 2017.\n",
    "\n",
    "        A. van den Oord and N. Kalchbrenner and O. Vinyals and L. Espeholt and A. Graves and K. Kavukcuoglu\n",
    "        Conditional Image Generation with PixelCNN Decoders, 2016\n",
    "        NIPS'16 Proceedings of the 30th International Conference on Neural Information Processing Systems\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class FullGatedConv2D(Conv2D):\n",
    "    \"\"\"Gated Convolutional Class\"\"\"\n",
    "\n",
    "    def __init__(self, filters, **kwargs):\n",
    "        super(FullGatedConv2D, self).__init__(filters=filters * 2, **kwargs)\n",
    "        self.nb_filters = filters\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Apply gated convolution\"\"\"\n",
    "\n",
    "        output = super(FullGatedConv2D, self).call(inputs)\n",
    "        linear = Activation(\"linear\")(output[:, :, :, :self.nb_filters])\n",
    "        sigmoid = Activation(\"sigmoid\")(output[:, :, :, self.nb_filters:])\n",
    "\n",
    "        return Multiply()([linear, sigmoid])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"Compute shape of layer output\"\"\"\n",
    "\n",
    "        output_shape = super(FullGatedConv2D, self).compute_output_shape(input_shape)\n",
    "        return tuple(output_shape[:3]) + (self.nb_filters * 2,)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"Return the config of the layer\"\"\"\n",
    "\n",
    "        config = super(FullGatedConv2D, self).get_config()\n",
    "        config['nb_filters'] = self.nb_filters\n",
    "        del config['filters']\n",
    "        return config\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Tensorflow Keras layer implementation of the octave convolution.\n",
    "\n",
    "Reference (based):\n",
    "    Yunpeng Chen, Haoqi Fan, Bing Xu, Zhicheng Yan, Yannis Kalantidis, Marcus Rohrbach, Shuicheng Yan, Jiashi Feng.\n",
    "    Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution.\n",
    "\n",
    "    OctConv-TFKeras\n",
    "    Github: https://github.com/koshian2/OctConv-TFKeras\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class OctConv2D(Layer):\n",
    "    \"\"\"Octave Convolutional Class\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 filters,\n",
    "                 alpha,\n",
    "                 kernel_size=(3,3),\n",
    "                 strides=(1,1),\n",
    "                 padding=\"same\",\n",
    "                 kernel_initializer=\"glorot_uniform\",\n",
    "                 kernel_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 **kwargs):\n",
    "        assert alpha >= 0 and alpha <= 1\n",
    "        assert filters > 0 and isinstance(filters, int)\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.filters = filters\n",
    "        # optional values\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "        self.kernel_constraint = kernel_constraint\n",
    "\n",
    "        # --> low channels\n",
    "        self.low_channels = int(self.filters * self.alpha)\n",
    "        # --> high channels\n",
    "        self.high_channels = self.filters - self.low_channels\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        assert len(input_shape[0]) == 4 and len(input_shape[1]) == 4\n",
    "        # assertion for high inputs\n",
    "        assert input_shape[0][1] // 2 >= self.kernel_size[0]\n",
    "        assert input_shape[0][2] // 2 >= self.kernel_size[1]\n",
    "        # assertion for low inputs\n",
    "        assert input_shape[0][1] // input_shape[1][1] == 2\n",
    "        assert input_shape[0][2] // input_shape[1][2] == 2\n",
    "\n",
    "        assert K.image_data_format() == \"channels_last\"\n",
    "        high_in = int(input_shape[0][3])\n",
    "        low_in = int(input_shape[1][3])\n",
    "\n",
    "        # High -> Low\n",
    "        self.high_to_high_kernel = self.add_weight(name=\"high_to_high_kernel\",\n",
    "                                                   shape=(*self.kernel_size, high_in, self.high_channels),\n",
    "                                                   initializer=self.kernel_initializer,\n",
    "                                                   regularizer=self.kernel_regularizer,\n",
    "                                                   constraint=self.kernel_constraint)\n",
    "        # High -> Low\n",
    "        self.high_to_low_kernel = self.add_weight(name=\"high_to_low_kernel\",\n",
    "                                                  shape=(*self.kernel_size, high_in, self.low_channels),\n",
    "                                                  initializer=self.kernel_initializer,\n",
    "                                                  regularizer=self.kernel_regularizer,\n",
    "                                                  constraint=self.kernel_constraint)\n",
    "\n",
    "        # Low -> High\n",
    "        self.low_to_high_kernel = self.add_weight(name=\"low_to_high_kernel\",\n",
    "                                                  shape=(*self.kernel_size, low_in, self.high_channels),\n",
    "                                                  initializer=self.kernel_initializer,\n",
    "                                                  regularizer=self.kernel_regularizer,\n",
    "                                                  constraint=self.kernel_constraint)\n",
    "        # Low -> Low\n",
    "        self.low_to_low_kernel = self.add_weight(name=\"low_to_low_kernel\",\n",
    "                                                 shape=(*self.kernel_size, low_in, self.low_channels),\n",
    "                                                 initializer=self.kernel_initializer,\n",
    "                                                 regularizer=self.kernel_regularizer,\n",
    "                                                 constraint=self.kernel_constraint)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Input=[x^H, x^L]\n",
    "        assert len(inputs) == 2\n",
    "        high_input, low_input = inputs\n",
    "        # High -> High conv\n",
    "        high_to_high = K.conv2d(high_input, self.high_to_high_kernel,\n",
    "                                strides=self.strides, padding=self.padding,\n",
    "                                data_format=\"channels_last\")\n",
    "        # High -> low conv\n",
    "        high_to_low = K.pool2d(high_input, (2, 2), strides=(2, 2), pool_mode=\"avg\")\n",
    "        high_to_low = K.conv2d(high_to_low, self.high_to_low_kernel, strides=self.strides,\n",
    "                               padding=self.padding, data_format=\"channels_last\")\n",
    "\n",
    "        # Low -> high conv\n",
    "        low_to_high = K.conv2d(low_input, self.low_to_high_kernel,\n",
    "                               strides=self.strides, padding=self.padding,\n",
    "                               data_format=\"channels_last\")\n",
    "        low_to_high = K.repeat_elements(low_to_high, 2, axis=1)\n",
    "        low_to_high = K.repeat_elements(low_to_high, 2, axis=2)\n",
    "\n",
    "        # Low -> low conv\n",
    "        low_to_low = K.conv2d(low_input, self.low_to_low_kernel,\n",
    "                              strides=self.strides, padding=self.padding,\n",
    "                              data_format=\"channels_last\")\n",
    "\n",
    "        # cross add\n",
    "        high_add = high_to_high + low_to_high\n",
    "        low_add = low_to_low + high_to_low\n",
    "\n",
    "        return [high_add, low_add]\n",
    "\n",
    "    def compute_output_shape(self, input_shapes):\n",
    "        high_in_shape, low_in_shape = input_shapes\n",
    "        high_out_shape = (*high_in_shape[:3], self.high_channels)\n",
    "        low_out_shape = (*low_in_shape[:3], self.low_channels)\n",
    "        return [high_out_shape, low_out_shape]\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        out_config = {\n",
    "            **base_config,\n",
    "            \"filters\": self.filters,\n",
    "            \"alpha\": self.alpha,\n",
    "            \"filters\": self.filters,\n",
    "            \"kernel_size\": self.kernel_size,\n",
    "            \"strides\": self.strides,\n",
    "            \"padding\": self.padding,\n",
    "            \"kernel_initializer\": self.kernel_initializer,\n",
    "            \"kernel_regularizer\": self.kernel_regularizer,\n",
    "            \"kernel_constraint\": self.kernel_constraint,\n",
    "        }\n",
    "        return out_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a48ed4fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T15:33:03.709658Z",
     "iopub.status.busy": "2023-05-01T15:33:03.709032Z",
     "iopub.status.idle": "2023-05-01T15:33:03.718972Z",
     "shell.execute_reply": "2023-05-01T15:33:03.717965Z",
     "shell.execute_reply.started": "2023-05-01T15:33:03.709615Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "# \n",
    "# from network.layers import FullGatedConv2D, GatedConv2D, OctConv2D\n",
    "from tensorflow.keras.layers import Conv2D, Bidirectional, LSTM, GRU, Dense\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization, LeakyReLU, PReLU\n",
    "from tensorflow.keras.layers import Input, Add, Activation, Lambda, MaxPooling2D, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c56f37c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T15:33:03.721015Z",
     "iopub.status.busy": "2023-05-01T15:33:03.720644Z",
     "iopub.status.idle": "2023-05-01T15:33:13.607627Z",
     "shell.execute_reply": "2023-05-01T15:33:13.606214Z",
     "shell.execute_reply.started": "2023-05-01T15:33:03.720973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaldiio in /opt/conda/lib/python3.7/site-packages (2.18.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from kaldiio) (1.21.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install kaldiio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "66042da1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T15:33:13.612739Z",
     "iopub.status.busy": "2023-05-01T15:33:13.612224Z",
     "iopub.status.idle": "2023-05-01T15:33:13.634275Z",
     "shell.execute_reply": "2023-05-01T15:33:13.633097Z",
     "shell.execute_reply.started": "2023-05-01T15:33:13.612688Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Language Model class.\n",
    "Create and read the corpus with the language model file.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "from kaldiio import WriteHelper\n",
    "\n",
    "\n",
    "class LanguageModel():\n",
    "\n",
    "    def __init__(self, output, N=3):\n",
    "        self.output_path = os.path.join(output, \"language\")\n",
    "        self.N = N\n",
    "\n",
    "    def generate_kaldi_assets(self, dtgen, predicts):\n",
    "        # get data and ground truth lists\n",
    "        ctc_TK, space_TK, ground_truth = \"<ctc>\", \"<space>\", []\n",
    "\n",
    "        for pt in ['train', 'valid', 'test']:\n",
    "            for x in dtgen.dataset[pt]['gt']:\n",
    "                ground_truth.append([space_TK if y == \" \" else y for y in list(f\" {x} \")])\n",
    "\n",
    "        # define dataset size and default tokens\n",
    "        ds_size = dtgen.size['train'] + dtgen.size['valid'] + dtgen.size['test']\n",
    "\n",
    "        # get chars list and save with the ctc and space tokens\n",
    "        chars = list(dtgen.tokenizer.chars) + [ctc_TK]\n",
    "        chars[chars.index(\" \")] = space_TK\n",
    "\n",
    "        kaldi_path = os.path.join(self.output_path, \"kaldi\")\n",
    "        os.makedirs(kaldi_path, exist_ok=True)\n",
    "\n",
    "        with open(os.path.join(kaldi_path, \"chars.lst\"), \"w\") as lg:\n",
    "            lg.write(\"\\n\".join(chars))\n",
    "\n",
    "        ark_file_name = os.path.join(kaldi_path, \"conf_mats.ark\")\n",
    "        scp_file_name = os.path.join(kaldi_path, \"conf_mats.scp\")\n",
    "\n",
    "        # save ark and scp file (laia output/kaldi input format)\n",
    "        with WriteHelper(f\"ark,scp:{ark_file_name},{scp_file_name}\") as writer:\n",
    "            for i, item in enumerate(predicts):\n",
    "                writer(str(i + ds_size), item)\n",
    "\n",
    "        # save ground_truth.lst file with sparse sentences\n",
    "        with open(os.path.join(kaldi_path, \"ground_truth.lst\"), \"w\") as lg:\n",
    "            for i, item in enumerate(ground_truth):\n",
    "                lg.write(f\"{i} {' '.join(item)}\\n\")\n",
    "\n",
    "        # save indexes of the train/valid and test partitions\n",
    "        with open(os.path.join(kaldi_path, \"ID_train.lst\"), \"w\") as lg:\n",
    "            range_index = [str(i) for i in range(0, ds_size - dtgen.size['test'])]\n",
    "            lg.write(\"\\n\".join(range_index))\n",
    "\n",
    "        with open(os.path.join(kaldi_path, \"ID_test.lst\"), \"w\") as lg:\n",
    "            range_index = [str(i) for i in range(ds_size - dtgen.size['test'], ds_size)]\n",
    "            lg.write(\"\\n\".join(range_index))\n",
    "\n",
    "    def kaldi(self, predict=True):\n",
    "        \"\"\"\n",
    "        Kaldi Speech Recognition Toolkit with SRI Language Modeling Toolkit.\n",
    "        ** Important Note **\n",
    "        You'll need to do all by yourself:\n",
    "        1. Compile Kaldi with SRILM and OpenBLAS.\n",
    "        2. Create and add kaldi folder in the project `lib` folder (``src/lib/kaldi/``)\n",
    "        3. Generate files (search `--kaldi_assets` in https://github.com/arthurflor23/handwritten-text-recognition):\n",
    "            a. `chars.lst`\n",
    "            b. `conf_mats.ark`\n",
    "            c. `ground_truth.lst`\n",
    "            d. `ID_test.lst`\n",
    "            e. `ID_train.lst`\n",
    "        4. Add files (item 3) in the project `output` folder: ``output/<DATASET>/kaldi/``\n",
    "        More information (maybe help) in ``src/lib/kaldi-decode-script.sh`` comments.\n",
    "        References:\n",
    "            D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann,\n",
    "            P. Motlicek, Y. Qian, P. Schwarz, J. Silovsky, G. Stem- mer and K. Vesely.\n",
    "            The Kaldi speech recognition toolkit, 2011.\n",
    "            Workshop on Automatic Speech Recognition and Understanding.\n",
    "            URL: http://github.com/kaldi-asr/kaldi\n",
    "            Andreas Stolcke.\n",
    "            SRILM - An Extensible Language Modeling Toolkit, 2002.\n",
    "            Proceedings of the 7th International Conference on Spoken Language Processing (ICSLP).\n",
    "            URL: http://www.speech.sri.com/projects/srilm/\n",
    "        \"\"\"\n",
    "\n",
    "        option = \"TEST\" if predict else \"TRAIN\"\n",
    "        output = os.path.join(self.output_path, \"kaldi\")\n",
    "\n",
    "        if os.system(f\"./language/kaldi-decode-script.sh {output} {option} {self.N}\") != 0:\n",
    "            print(\"\\n##################\\n\")\n",
    "            print(\"Kaldi script error.\")\n",
    "            print(\"\\n##################\\n\")\n",
    "\n",
    "        if predict:\n",
    "            predicts = open(os.path.join(output, \"data\", \"predicts_t\")).read().splitlines()\n",
    "\n",
    "            for i, line in enumerate(predicts):\n",
    "                tokens = line.split()\n",
    "                predicts[i] = \"\".join(tokens[1:]).replace(\"<space>\", \" \").strip()\n",
    "\n",
    "            return predicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4bcfaac4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T15:33:13.638683Z",
     "iopub.status.busy": "2023-05-01T15:33:13.638472Z",
     "iopub.status.idle": "2023-05-01T15:33:13.734380Z",
     "shell.execute_reply": "2023-05-01T15:33:13.733180Z",
     "shell.execute_reply.started": "2023-05-01T15:33:13.638655Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Handwritten Text Recognition Neural Network\"\"\"\n",
    "\n",
    "import os\n",
    "import logging\n",
    "\n",
    "try:\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\"\n",
    "    logging.disable(logging.WARNING)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from tensorflow.keras.callbacks import CSVLogger, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "# from network.layers import FullGatedConv2D, GatedConv2D, OctConv2D\n",
    "from tensorflow.keras.layers import Conv2D, Bidirectional, LSTM, GRU, Dense\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization, LeakyReLU, PReLU\n",
    "from tensorflow.keras.layers import Input, Add, Activation, Lambda, MaxPooling2D, Reshape\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "HTRModel Class based on:\n",
    "    Y. Soullard, C. Ruffino and T. Paquet,\n",
    "    CTCModel: A Connectionnist Temporal Classification implementation for Keras.\n",
    "    ee: https://arxiv.org/abs/1901.07957, 2019.\n",
    "    github: https://github.com/ysoullard/HTRModel\n",
    "\n",
    "\n",
    "The HTRModel class use Tensorflow 2 Keras module for the use of the\n",
    "Connectionist Temporal Classification (CTC) with the Hadwritten Text Recognition (HTR).\n",
    "\n",
    "In a Tensorflow Keras Model, x is the input features and y the labels.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class HTRModel:\n",
    "\n",
    "    def __init__(self,\n",
    "                 architecture,\n",
    "                 input_size,\n",
    "                 vocab_size,\n",
    "                 greedy=False,\n",
    "                 beam_width=10,\n",
    "                 top_paths=1,\n",
    "                 stop_tolerance=20,\n",
    "                 reduce_tolerance=15,\n",
    "                 reduce_factor=0.1,\n",
    "                 reduce_cooldown=0):\n",
    "        \"\"\"\n",
    "        Initialization of a HTR Model.\n",
    "\n",
    "        :param\n",
    "            architecture: option of the architecture model to build and compile\n",
    "            greedy, beam_width, top_paths: Parameters of the CTC decoding\n",
    "            (see ctc decoding tensorflow for more details)\n",
    "        \"\"\"\n",
    "\n",
    "        self.architecture = globals()[architecture]\n",
    "        self.input_size = input_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.model = None\n",
    "        self.greedy = greedy\n",
    "        self.beam_width = beam_width\n",
    "        self.top_paths = max(1, top_paths)\n",
    "\n",
    "        self.stop_tolerance = stop_tolerance\n",
    "        self.reduce_tolerance = reduce_tolerance\n",
    "        self.reduce_factor = reduce_factor\n",
    "        self.reduce_cooldown = reduce_cooldown\n",
    "\n",
    "    def summary(self, output=None, target=None):\n",
    "        \"\"\"Show/Save model structure (summary)\"\"\"\n",
    "\n",
    "        self.model.summary()\n",
    "\n",
    "        if target is not None:\n",
    "            os.makedirs(output, exist_ok=True)\n",
    "\n",
    "            with open(os.path.join(output, target), \"w\") as f:\n",
    "                with redirect_stdout(f):\n",
    "                    self.model.summary()\n",
    "\n",
    "    def load_checkpoint(self, target):\n",
    "        \"\"\" Load a model with checkpoint file\"\"\"\n",
    "\n",
    "        if os.path.isfile(target):\n",
    "            if self.model is None:\n",
    "                self.compile()\n",
    "\n",
    "            self.model.load_weights(target)\n",
    "\n",
    "    def get_callbacks(self, logdir, checkpoint, monitor=\"val_loss\", verbose=0):\n",
    "        \"\"\"Setup the list of callbacks for the model\"\"\"\n",
    "\n",
    "        callbacks = [\n",
    "            CSVLogger(\n",
    "                filename=os.path.join(logdir, \"epochs.log\"),\n",
    "                separator=\";\",\n",
    "                append=True),\n",
    "            TensorBoard(\n",
    "                log_dir=logdir,\n",
    "                histogram_freq=10,\n",
    "                profile_batch=0,\n",
    "                write_graph=True,\n",
    "                write_images=False,\n",
    "                update_freq=\"epoch\"),\n",
    "            ModelCheckpoint(\n",
    "                filepath=checkpoint,\n",
    "                monitor=monitor,\n",
    "                save_best_only=True,\n",
    "                save_weights_only=True,\n",
    "                verbose=verbose),\n",
    "            EarlyStopping(\n",
    "                monitor=monitor,\n",
    "                min_delta=1e-8,\n",
    "                patience=self.stop_tolerance,\n",
    "                restore_best_weights=True,\n",
    "                verbose=verbose),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor=monitor,\n",
    "                min_delta=1e-8,\n",
    "                factor=self.reduce_factor,\n",
    "                patience=self.reduce_tolerance,\n",
    "                cooldown=self.reduce_cooldown,\n",
    "                verbose=verbose)\n",
    "        ]\n",
    "\n",
    "        return callbacks\n",
    "\n",
    "    def compile(self, learning_rate=None, initial_step=0):\n",
    "        \"\"\"\n",
    "        Configures the HTR Model for training/predict.\n",
    "\n",
    "        :param optimizer: optimizer for training\n",
    "        \"\"\"\n",
    "\n",
    "        # define inputs, outputs and optimizer of the chosen architecture\n",
    "        inputs, outputs = self.architecture(self.input_size, self.vocab_size + 1)\n",
    "\n",
    "        if learning_rate is None:\n",
    "            learning_rate = CustomSchedule(d_model=self.vocab_size + 1, initial_step=initial_step)\n",
    "            self.learning_schedule = True\n",
    "        else:\n",
    "            self.learning_schedule = False\n",
    "\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "        # create and compile\n",
    "        self.model = Model(inputs=inputs, outputs=outputs)\n",
    "        self.model.compile(optimizer=optimizer, loss=self.ctc_loss_lambda_func)\n",
    "\n",
    "    def fit(self,\n",
    "            x=None,\n",
    "            y=None,\n",
    "            batch_size=None,\n",
    "            epochs=1,\n",
    "            verbose=1,\n",
    "            callbacks=None,\n",
    "            validation_split=0.0,\n",
    "            validation_data=None,\n",
    "            shuffle=True,\n",
    "            class_weight=None,\n",
    "            sample_weight=None,\n",
    "            initial_epoch=0,\n",
    "            steps_per_epoch=None,\n",
    "            validation_steps=None,\n",
    "            validation_freq=1,\n",
    "            max_queue_size=10,\n",
    "            workers=1,\n",
    "            use_multiprocessing=False,\n",
    "            **kwargs):\n",
    "        \"\"\"\n",
    "        Model training on data yielded (fit function has support to generator).\n",
    "        A fit() abstration function of TensorFlow 2.\n",
    "\n",
    "        Provide x parameter of the form: yielding (x, y, sample_weight).\n",
    "\n",
    "        :param: See tensorflow.keras.Model.fit()\n",
    "        :return: A history object\n",
    "        \"\"\"\n",
    "\n",
    "        # remove ReduceLROnPlateau (if exist) when use schedule learning rate\n",
    "        if callbacks and self.learning_schedule:\n",
    "            callbacks = [x for x in callbacks if not isinstance(x, ReduceLROnPlateau)]\n",
    "\n",
    "        out = self.model.fit(x=x, y=y, batch_size=batch_size, epochs=epochs, verbose=verbose,\n",
    "                             callbacks=callbacks, validation_split=validation_split,\n",
    "                             validation_data=validation_data, shuffle=shuffle,\n",
    "                             class_weight=class_weight, sample_weight=sample_weight,\n",
    "                             initial_epoch=initial_epoch, steps_per_epoch=steps_per_epoch,\n",
    "                             validation_steps=validation_steps, validation_freq=validation_freq,\n",
    "                             max_queue_size=max_queue_size, workers=workers,\n",
    "                             use_multiprocessing=use_multiprocessing, **kwargs)\n",
    "        return out\n",
    "\n",
    "    def predict(self,\n",
    "                x,\n",
    "                batch_size=None,\n",
    "                verbose=0,\n",
    "                steps=1,\n",
    "                callbacks=None,\n",
    "                max_queue_size=10,\n",
    "                workers=1,\n",
    "                use_multiprocessing=False,\n",
    "                ctc_decode=True):\n",
    "        \"\"\"\n",
    "        Model predicting on data yielded (predict function has support to generator).\n",
    "        A predict() abstration function of TensorFlow 2.\n",
    "\n",
    "        Provide x parameter of the form: yielding [x].\n",
    "\n",
    "        :param: See tensorflow.keras.Model.predict()\n",
    "        :return: raw data on `ctc_decode=False` or CTC decode on `ctc_decode=True` (both with probabilities)\n",
    "        \"\"\"\n",
    "\n",
    "        if verbose == 1:\n",
    "            print(\"Model Predict\")\n",
    "\n",
    "        out = self.model.predict(x=x, batch_size=batch_size, verbose=verbose, steps=steps,\n",
    "                                 callbacks=callbacks, max_queue_size=max_queue_size,\n",
    "                                 workers=workers, use_multiprocessing=use_multiprocessing)\n",
    "\n",
    "        if not ctc_decode:\n",
    "            return np.log(out.clip(min=1e-8)), []\n",
    "\n",
    "        steps_done = 0\n",
    "        if verbose == 1:\n",
    "            print(\"CTC Decode\")\n",
    "            progbar = tf.keras.utils.Progbar(target=steps)\n",
    "\n",
    "        batch_size = int(np.ceil(len(out) / steps))\n",
    "        input_length = len(max(out, key=len))\n",
    "\n",
    "        predicts, probabilities = [], []\n",
    "\n",
    "        while steps_done < steps:\n",
    "            index = steps_done * batch_size\n",
    "            until = index + batch_size\n",
    "\n",
    "            x_test = np.asarray(out[index:until])\n",
    "            x_test_len = np.asarray([input_length for _ in range(len(x_test))])\n",
    "\n",
    "            decode, log = K.ctc_decode(x_test,\n",
    "                                       x_test_len,\n",
    "                                       greedy=self.greedy,\n",
    "                                       beam_width=self.beam_width,\n",
    "                                       top_paths=self.top_paths)\n",
    "\n",
    "            probabilities.extend([np.exp(x) for x in log])\n",
    "            decode = [[[int(p) for p in x if p != -1] for x in y] for y in decode]\n",
    "            predicts.extend(np.swapaxes(decode, 0, 1))\n",
    "\n",
    "            steps_done += 1\n",
    "            if verbose == 1:\n",
    "                progbar.update(steps_done)\n",
    "\n",
    "        return (predicts, probabilities)\n",
    "\n",
    "    @staticmethod\n",
    "    def ctc_loss_lambda_func(y_true, y_pred):\n",
    "        \"\"\"Function for computing the CTC loss\"\"\"\n",
    "\n",
    "        if len(y_true.shape) > 2:\n",
    "            y_true = tf.squeeze(y_true)\n",
    "\n",
    "        # y_pred.shape = (batch_size, string_length, alphabet_size_1_hot_encoded)\n",
    "        # output of every model is softmax\n",
    "        # so sum across alphabet_size_1_hot_encoded give 1\n",
    "        #               string_length give string length\n",
    "        input_length = tf.math.reduce_sum(y_pred, axis=-1, keepdims=False)\n",
    "        input_length = tf.math.reduce_sum(input_length, axis=-1, keepdims=True)\n",
    "\n",
    "        # y_true strings are padded with 0\n",
    "        # so sum of non-zero gives number of characters in this string\n",
    "        label_length = tf.math.count_nonzero(y_true, axis=-1, keepdims=True, dtype=\"int64\")\n",
    "\n",
    "        loss = K.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "\n",
    "        # average loss across all entries in the batch\n",
    "        loss = tf.reduce_mean(loss)\n",
    "\n",
    "        return loss\n",
    "    def save_model(self, filepath):\n",
    "        tf.keras.models.save_model(self.model, filepath)\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Custom Schedule\n",
    "\n",
    "Reference:\n",
    "    Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and\n",
    "    Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin.\n",
    "    \"Attention Is All You Need\", 2017\n",
    "    arXiv, URL: https://arxiv.org/abs/1706.03762\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    Custom schedule of the learning rate with warmup_steps.\n",
    "    From original paper \"Attention is all you need\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, initial_step=0, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, dtype=\"float32\")\n",
    "        self.initial_step = initial_step\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step + self.initial_step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Networks to the Handwritten Text Recognition Model\n",
    "\n",
    "Reference:\n",
    "    Moysset, B. and Messina, R.:\n",
    "    Are 2D-LSTM really dead for offline text recognition?\n",
    "    In: International Journal on Document Analysis and Recognition (IJDAR)\n",
    "    Springer Science and Business Media LLC\n",
    "    URL: http://dx.doi.org/10.1007/s10032-019-00325-0\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def bluche(input_size, d_model):\n",
    "    \"\"\"\n",
    "    Gated Convolucional Recurrent Neural Network by Bluche et al.\n",
    "\n",
    "    Reference:\n",
    "        Bluche, T., Messina, R.:\n",
    "        Gated convolutional recurrent neural networks for multilingual handwriting recognition.\n",
    "        In: Document Analysis and Recognition (ICDAR), 2017\n",
    "        14th IAPR International Conference on, vol. 1, pp. 646–651, 2017.\n",
    "        URL: https://ieeexplore.ieee.org/document/8270042\n",
    "    \"\"\"\n",
    "\n",
    "    input_data = Input(name=\"input\", shape=input_size)\n",
    "    cnn = Reshape((input_size[0] // 2, input_size[1] // 2, input_size[2] * 4))(input_data)\n",
    "\n",
    "    cnn = Conv2D(filters=8, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", activation=\"tanh\")(cnn)\n",
    "\n",
    "    cnn = Conv2D(filters=16, kernel_size=(2, 4), strides=(2, 4), padding=\"same\", activation=\"tanh\")(cnn)\n",
    "    cnn = GatedConv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(cnn)\n",
    "\n",
    "    cnn = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", activation=\"tanh\")(cnn)\n",
    "    cnn = GatedConv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(cnn)\n",
    "\n",
    "    cnn = Conv2D(filters=64, kernel_size=(2, 4), strides=(2, 4), padding=\"same\", activation=\"tanh\")(cnn)\n",
    "    cnn = GatedConv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(cnn)\n",
    "\n",
    "    cnn = Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", activation=\"tanh\")(cnn)\n",
    "    #cnn = MaxPooling2D(pool_size=(1, 4), strides=(1, 4), padding=\"valid\")(cnn)\n",
    "\n",
    "    shape = cnn.get_shape()\n",
    "    blstm = Reshape((shape[1], shape[2] * shape[3]))(cnn)\n",
    "\n",
    "    blstm = Bidirectional(LSTM(units=128, return_sequences=True))(blstm)\n",
    "    blstm = Dense(units=128, activation=\"tanh\")(blstm)\n",
    "\n",
    "    blstm = Bidirectional(LSTM(units=128, return_sequences=True))(blstm)\n",
    "    output_data = Dense(units=d_model, activation=\"softmax\")(blstm)\n",
    "\n",
    "    return (input_data, output_data)\n",
    "\n",
    "\n",
    "def puigcerver(input_size, d_model):\n",
    "    \"\"\"\n",
    "    Convolucional Recurrent Neural Network by Puigcerver et al.\n",
    "\n",
    "    Reference:\n",
    "        Joan Puigcerver.\n",
    "        Are multidimensional recurrent layers really necessary for handwritten text recognition?\n",
    "        In: Document Analysis and Recognition (ICDAR), 2017 14th\n",
    "        IAPR International Conference on, vol. 1, pp. 67–72. IEEE (2017)\n",
    "\n",
    "        Carlos Mocholí Calvo and Enrique Vidal Ruiz.\n",
    "        Development and experimentation of a deep learning system for convolutional and recurrent neural networks\n",
    "        Escola Tècnica Superior d’Enginyeria Informàtica, Universitat Politècnica de València, 2018\n",
    "    \"\"\"\n",
    "\n",
    "    input_data = Input(name=\"input\", shape=input_size)\n",
    "\n",
    "    cnn = Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(input_data)\n",
    "    cnn = BatchNormalization()(cnn)\n",
    "    cnn = LeakyReLU(alpha=0.01)(cnn)\n",
    "    cnn = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")(cnn)\n",
    "\n",
    "    cnn = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(cnn)\n",
    "    cnn = BatchNormalization()(cnn)\n",
    "    cnn = LeakyReLU(alpha=0.01)(cnn)\n",
    "    cnn = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")(cnn)\n",
    "\n",
    "    cnn = Dropout(rate=0.2)(cnn)\n",
    "    cnn = Conv2D(filters=48, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(cnn)\n",
    "    cnn = BatchNormalization()(cnn)\n",
    "    cnn = LeakyReLU(alpha=0.01)(cnn)\n",
    "    cnn = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")(cnn)\n",
    "\n",
    "    cnn = Dropout(rate=0.2)(cnn)\n",
    "    cnn = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(cnn)\n",
    "    cnn = BatchNormalization()(cnn)\n",
    "    cnn = LeakyReLU(alpha=0.01)(cnn)\n",
    "\n",
    "    cnn = Dropout(rate=0.2)(cnn)\n",
    "    cnn = Conv2D(filters=80, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(cnn)\n",
    "    cnn = BatchNormalization()(cnn)\n",
    "    cnn = LeakyReLU(alpha=0.01)(cnn)\n",
    "\n",
    "    shape = cnn.get_shape()\n",
    "    blstm = Reshape((shape[1], shape[2] * shape[3]))(cnn)\n",
    "\n",
    "    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
    "    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
    "    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
    "    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
    "    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
    "\n",
    "    blstm = Dropout(rate=0.5)(blstm)\n",
    "    output_data = Dense(units=d_model, activation=\"softmax\")(blstm)\n",
    "\n",
    "    return (input_data, output_data)\n",
    "\n",
    "from tensorflow.keras.layers import Add\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def resnet_block(x, filters, kernel_size=(3, 3), kernel_regularizer=None):\n",
    "    shortcut = x\n",
    "\n",
    "    x = Conv2D(filters, kernel_size, padding=\"same\", kernel_initializer=\"he_uniform\", kernel_regularizer=kernel_regularizer)(x)\n",
    "    x = PReLU(shared_axes=[1, 2])(x)\n",
    "    x = BatchNormalization(renorm=True)(x)\n",
    "\n",
    "    x = Conv2D(filters, kernel_size, padding=\"same\", kernel_initializer=\"he_uniform\", kernel_regularizer=kernel_regularizer)(x)\n",
    "    x = BatchNormalization(renorm=True)(x)\n",
    "\n",
    "    x = Add()([shortcut, x])\n",
    "    x = PReLU(shared_axes=[1, 2])(x)\n",
    "\n",
    "    return x\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "def flor(input_size, d_model, l2_reg = 5e-4):\n",
    "    kernel_regularizer = l2(l2_reg)\n",
    "\n",
    "    input_data = Input(name=\"input\", shape=input_size)\n",
    "\n",
    "    cnn = Conv2D(filters=16, kernel_size=(3, 3), strides=(2, 2), padding=\"same\", kernel_initializer=\"he_uniform\")(input_data)\n",
    "    cnn = PReLU(shared_axes=[1, 2])(cnn)\n",
    "    cnn = BatchNormalization(renorm=True)(cnn)\n",
    "    cnn = FullGatedConv2D(filters=16, kernel_size=(3, 3), padding=\"same\")(cnn)\n",
    "\n",
    "    cnn = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", kernel_initializer=\"he_uniform\")(cnn)\n",
    "    cnn = PReLU(shared_axes=[1, 2])(cnn)\n",
    "    cnn = BatchNormalization(renorm=True)(cnn)\n",
    "    cnn = FullGatedConv2D(filters=32, kernel_size=(3, 3), padding=\"same\")(cnn)\n",
    "\n",
    "    cnn = Conv2D(filters=40, kernel_size=(2, 4), strides=(2, 4), padding=\"same\", kernel_initializer=\"he_uniform\")(cnn)\n",
    "    cnn = PReLU(shared_axes=[1, 2])(cnn)\n",
    "    cnn = BatchNormalization(renorm=True)(cnn)\n",
    "    cnn = FullGatedConv2D(filters=40, kernel_size=(3, 3), padding=\"same\", kernel_constraint=MaxNorm(4, [0, 1, 2]))(cnn)\n",
    "    cnn = Dropout(rate=0.2, noise_shape=(None, 1, 1, 40))(cnn)\n",
    "\n",
    "\n",
    "    cnn = Conv2D(filters=48, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", kernel_initializer=\"he_uniform\")(cnn)\n",
    "    cnn = PReLU(shared_axes=[1, 2])(cnn)\n",
    "    cnn = BatchNormalization(renorm=True)(cnn)\n",
    "    cnn = FullGatedConv2D(filters=48, kernel_size=(3, 3), padding=\"same\", kernel_constraint=MaxNorm(4, [0, 1, 2]))(cnn)\n",
    "    cnn = Dropout(rate=0.2, noise_shape=(None, 1, 1, 48))(cnn)\n",
    "\n",
    "\n",
    "    cnn = Conv2D(filters=56, kernel_size=(2, 4), strides=(2, 4), padding=\"same\", kernel_initializer=\"he_uniform\")(cnn)\n",
    "    cnn = PReLU(shared_axes=[1, 2])(cnn)\n",
    "    cnn = BatchNormalization()(cnn)\n",
    "\n",
    "    cnn = FullGatedConv2D(filters=56, kernel_size=(3, 3), padding=\"same\", kernel_constraint=MaxNorm(4, [0, 1, 2]))(cnn)\n",
    "    cnn = Dropout(rate=0.2, noise_shape=(None, 1, 1, 56))(cnn)\n",
    "\n",
    "\n",
    "    cnn = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", kernel_initializer=\"he_uniform\", kernel_regularizer=kernel_regularizer)(cnn)\n",
    "    cnn = PReLU(shared_axes=[1, 2])(cnn)\n",
    "    cnn = BatchNormalization(renorm=True)(cnn)\n",
    "    cnn = resnet_block(cnn, filters=64, kernel_regularizer=kernel_regularizer)     \n",
    "\n",
    "    shape = cnn.get_shape()\n",
    "    bgru = Reshape((shape[1], shape[2] * shape[3]))(cnn)\n",
    "\n",
    "    bgru = Bidirectional(GRU(units=256, return_sequences=True, dropout=0.5))(bgru)\n",
    "    bgru = Dense(units=512)(bgru)\n",
    "\n",
    "    bgru = Bidirectional(GRU(units=256, return_sequences=True, dropout=0.5))(bgru)\n",
    "    bgru = Dense(units=512)(bgru)\n",
    "\n",
    "    bgru = Bidirectional(GRU(units=256, return_sequences=True, dropout=0.5))(bgru)\n",
    "\n",
    "    output_data = TimeDistributed(Dense(units=d_model, activation=\"softmax\"))(bgru)\n",
    "\n",
    "    return (input_data, output_data)\n",
    "\n",
    "\n",
    "def puigcerver_octconv(input_size, d_model):\n",
    "    \"\"\"\n",
    "    Octave CNN by khinggan, architecture is same as puigcerver\n",
    "    \"\"\"\n",
    "\n",
    "    alpha = 0.25\n",
    "    input_data = Input(name=\"input\", shape=input_size)\n",
    "    high = input_data\n",
    "    low = tf.keras.layers.AveragePooling2D(2)(input_data)\n",
    "\n",
    "    high, low = OctConv2D(filters=16, alpha=alpha)([high, low])\n",
    "    high = BatchNormalization()(high)\n",
    "    low = BatchNormalization()(low)\n",
    "    high = LeakyReLU(alpha=0.01)(high)\n",
    "    low = LeakyReLU(alpha=0.01)(low)\n",
    "    high = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")(high)\n",
    "    low = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")(low)\n",
    "\n",
    "    high, low = OctConv2D(filters=32, alpha=alpha)([high, low])\n",
    "    high = BatchNormalization()(high)\n",
    "    low = BatchNormalization()(low)\n",
    "    high = LeakyReLU(alpha=0.01)(high)\n",
    "    low = LeakyReLU(alpha=0.01)(low)\n",
    "    high = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")(high)\n",
    "    low = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")(low)\n",
    "\n",
    "    high = Dropout(rate=0.2)(high)\n",
    "    low = Dropout(rate=0.2)(low)\n",
    "    high = Conv2D(filters=48, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(high)\n",
    "    low = Conv2D(filters=48, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(low)\n",
    "    high = BatchNormalization()(high)\n",
    "    low = BatchNormalization()(low)\n",
    "    high = LeakyReLU(alpha=0.01)(high)\n",
    "    low = LeakyReLU(alpha=0.01)(low)\n",
    "    high = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")(high)\n",
    "    low = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")(low)\n",
    "\n",
    "    high = Dropout(rate=0.2)(high)\n",
    "    low = Dropout(rate=0.2)(low)\n",
    "    high = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(high)\n",
    "    low = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(low)\n",
    "    high = BatchNormalization()(high)\n",
    "    low = BatchNormalization()(low)\n",
    "    high = LeakyReLU(alpha=0.01)(high)\n",
    "    low = LeakyReLU(alpha=0.01)(low)\n",
    "\n",
    "    high = Dropout(rate=0.2)(high)\n",
    "    low = Dropout(rate=0.2)(low)\n",
    "    high = Conv2D(filters=80, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(high)\n",
    "    low = Conv2D(filters=80, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(low)\n",
    "    high = BatchNormalization()(high)\n",
    "    low = BatchNormalization()(low)\n",
    "    high = LeakyReLU(alpha=0.01)(high)\n",
    "    low = LeakyReLU(alpha=0.01)(low)\n",
    "\n",
    "    x = _create_octconv_last_block([high, low], 80, alpha)\n",
    "\n",
    "    shape = x.get_shape()\n",
    "    blstm = Reshape((shape[1], shape[2] * shape[3]))(x)\n",
    "\n",
    "    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
    "    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
    "    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
    "    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
    "    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
    "\n",
    "    blstm = Dropout(rate=0.5)(blstm)\n",
    "    output_data = Dense(units=d_model, activation=\"softmax\")(blstm)\n",
    "\n",
    "    return (input_data, output_data)\n",
    "\n",
    "\n",
    "def _create_octconv_last_block(inputs, ch, alpha):\n",
    "    high, low = inputs\n",
    "\n",
    "    high, low = OctConv2D(filters=ch, alpha=alpha)([high, low])\n",
    "    high = BatchNormalization()(high)\n",
    "    high = Activation(\"relu\")(high)\n",
    "\n",
    "    low = BatchNormalization()(low)\n",
    "    low = Activation(\"relu\")(low)\n",
    "\n",
    "    high_to_high = Conv2D(ch, 3, padding=\"same\")(high)\n",
    "    low_to_high = Conv2D(ch, 3, padding=\"same\")(low)\n",
    "    low_to_high = Lambda(lambda x: K.repeat_elements(K.repeat_elements(x, 2, axis=1), 2, axis=2))(low_to_high)\n",
    "\n",
    "    x = Add()([high_to_high, low_to_high])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8c55ab6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T15:33:13.736696Z",
     "iopub.status.busy": "2023-05-01T15:33:13.736394Z",
     "iopub.status.idle": "2023-05-01T16:06:02.413144Z",
     "shell.execute_reply": "2023-05-01T16:06:02.412126Z",
     "shell.execute_reply.started": "2023-05-01T15:33:13.736653Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HTRModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping, ReduceLROnPlateau\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# from network.model import HTRModel\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Create and compile HTRModel\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mHTRModel\u001b[49m(architecture\u001b[38;5;241m=\u001b[39mARCH,\n\u001b[0;32m      7\u001b[0m                  input_size\u001b[38;5;241m=\u001b[39mIMG_SIZE,\n\u001b[0;32m      8\u001b[0m                  vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m110\u001b[39m,\n\u001b[0;32m      9\u001b[0m                  beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     10\u001b[0m                  stop_tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m     11\u001b[0m                  reduce_tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m,\n\u001b[0;32m     12\u001b[0m                  reduce_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Early stopping and learning rate reduction callbacks\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'HTRModel' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# from network.model import HTRModel\n",
    "\n",
    "# Create and compile HTRModel\n",
    "model = HTRModel(architecture=ARCH,\n",
    "                 input_size=IMG_SIZE,\n",
    "                 vocab_size=110,\n",
    "                 beam_width=10,\n",
    "                 stop_tolerance=20,\n",
    "                 reduce_tolerance=15,\n",
    "                 reduce_factor=0.1)\n",
    "\n",
    "model.compile(learning_rate=0.0001)\n",
    "\n",
    "# Early stopping and learning rate reduction callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, restore_best_weights=True)\n",
    "reduce_lr_on_plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=15, verbose=1, min_lr=1e-6)\n",
    "\n",
    "callbacks = [early_stopping, reduce_lr_on_plateau]\n",
    "\n",
    "model.fit(x=test_datagen,\n",
    "                    \n",
    "                    epochs=5,\n",
    "                    verbose=1,\n",
    "                    validation_data=validation_datagen,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4528e16f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "267f59f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T16:17:24.974238Z",
     "iopub.status.busy": "2023-05-01T16:17:24.973369Z",
     "iopub.status.idle": "2023-05-01T16:17:25.011518Z",
     "shell.execute_reply": "2023-05-01T16:17:25.010440Z",
     "shell.execute_reply.started": "2023-05-01T16:17:24.974181Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 128)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "curr_data = test_data[56].split()\n",
    "curr_img_path = curr_data[0]\n",
    "\n",
    "\n",
    "curr_img_path = \"/\".join(curr_img_path.split(\"/\")[2:])\n",
    "curr_img_path = os.path.join(IMAGES_PATH, curr_img_path)\n",
    "curr_img = preprocess(curr_img_path, IMG_SIZE)\n",
    "curr_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "faa93b30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T16:17:26.106018Z",
     "iopub.status.busy": "2023-05-01T16:17:26.105733Z",
     "iopub.status.idle": "2023-05-01T16:17:26.353604Z",
     "shell.execute_reply": "2023-05-01T16:17:26.352184Z",
     "shell.execute_reply.started": "2023-05-01T16:17:26.105966Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x79997e3a4e90>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(curr_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "348dc4ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T16:17:26.723558Z",
     "iopub.status.busy": "2023-05-01T16:17:26.723274Z",
     "iopub.status.idle": "2023-05-01T16:17:26.732471Z",
     "shell.execute_reply": "2023-05-01T16:17:26.731439Z",
     "shell.execute_reply.started": "2023-05-01T16:17:26.723523Z"
    }
   },
   "outputs": [],
   "source": [
    "curr_img = np.transpose(curr_img, (1, 0))\n",
    "curr_img = np.expand_dims(curr_img, axis=-1)\n",
    "curr_img = np.expand_dims(curr_img, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "84ca3815",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T16:17:27.823305Z",
     "iopub.status.busy": "2023-05-01T16:17:27.822574Z",
     "iopub.status.idle": "2023-05-01T16:17:27.830471Z",
     "shell.execute_reply": "2023-05-01T16:17:27.829222Z",
     "shell.execute_reply.started": "2023-05-01T16:17:27.823259Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 128, 32, 1)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a439cfcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ce2156c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T16:17:30.042315Z",
     "iopub.status.busy": "2023-05-01T16:17:30.041747Z",
     "iopub.status.idle": "2023-05-01T16:17:33.091362Z",
     "shell.execute_reply": "2023-05-01T16:17:33.090353Z",
     "shell.execute_reply.started": "2023-05-01T16:17:30.042266Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[60]])], [array([4.544056e-09], dtype=float32)])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(curr_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c5d953",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-01T16:06:02.453251Z",
     "iopub.status.idle": "2023-05-01T16:06:02.453757Z",
     "shell.execute_reply": "2023-05-01T16:06:02.453527Z",
     "shell.execute_reply.started": "2023-05-01T16:06:02.453502Z"
    }
   },
   "outputs": [],
   "source": [
    "print(charl[57])\n",
    "print(charl[60])\n",
    "print(charl[3])\n",
    "print(charl[23])\n",
    "print(charl[21])\n",
    "print(charl[60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7081b0b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-01T16:06:02.455688Z",
     "iopub.status.idle": "2023-05-01T16:06:02.456559Z",
     "shell.execute_reply": "2023-05-01T16:06:02.456191Z",
     "shell.execute_reply.started": "2023-05-01T16:06:02.456147Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61e147f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-01T16:06:02.458726Z",
     "iopub.status.idle": "2023-05-01T16:06:02.459184Z",
     "shell.execute_reply": "2023-05-01T16:06:02.458943Z",
     "shell.execute_reply.started": "2023-05-01T16:06:02.458918Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(curr_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30325b6c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-01T16:06:02.460547Z",
     "iopub.status.idle": "2023-05-01T16:06:02.461638Z",
     "shell.execute_reply": "2023-05-01T16:06:02.461383Z",
     "shell.execute_reply.started": "2023-05-01T16:06:02.461350Z"
    }
   },
   "outputs": [],
   "source": [
    "print(charl[57])\n",
    "print(charl[60])\n",
    "print(charl[3])\n",
    "print(charl[23])\n",
    "print(charl[21])\n",
    "print(charl[60])\n",
    "print(charl[3])\n",
    "print(charl[23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08d35bc",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-01T16:06:02.463321Z",
     "iopub.status.idle": "2023-05-01T16:06:02.463782Z",
     "shell.execute_reply": "2023-05-01T16:06:02.463552Z",
     "shell.execute_reply.started": "2023-05-01T16:06:02.463527Z"
    }
   },
   "outputs": [],
   "source": [
    "xt=test_datagen[0][0]\n",
    "yt=test_datagen[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c10a20e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-01T16:06:02.465731Z",
     "iopub.status.idle": "2023-05-01T16:06:02.466205Z",
     "shell.execute_reply": "2023-05-01T16:06:02.465953Z",
     "shell.execute_reply.started": "2023-05-01T16:06:02.465927Z"
    }
   },
   "outputs": [],
   "source": [
    "predicts, _ = model.predict(x=xt,\n",
    "                            ctc_decode=True,\n",
    "                            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d87d2f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-01T16:06:02.467767Z",
     "iopub.status.idle": "2023-05-01T16:06:02.468574Z",
     "shell.execute_reply": "2023-05-01T16:06:02.468326Z",
     "shell.execute_reply.started": "2023-05-01T16:06:02.468299Z"
    }
   },
   "outputs": [],
   "source": [
    "len(predicts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3049580",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-01T16:06:02.470483Z",
     "iopub.status.idle": "2023-05-01T16:06:02.471323Z",
     "shell.execute_reply": "2023-05-01T16:06:02.471070Z",
     "shell.execute_reply.started": "2023-05-01T16:06:02.471043Z"
    }
   },
   "outputs": [],
   "source": [
    "predicts[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb211ca0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-01T16:06:02.472856Z",
     "iopub.status.idle": "2023-05-01T16:06:02.473689Z",
     "shell.execute_reply": "2023-05-01T16:06:02.473453Z",
     "shell.execute_reply.started": "2023-05-01T16:06:02.473425Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert(predicts):\n",
    "    a=[]\n",
    "    b=[]\n",
    "    for i in range(len(predicts)):\n",
    "        b=[]\n",
    "        for j in range(len(predicts[i][0])):\n",
    "            b.append(charl[predicts[i][0][j]])\n",
    "        a.append(b)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4684b0df",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-01T16:06:02.475408Z",
     "iopub.status.idle": "2023-05-01T16:06:02.476277Z",
     "shell.execute_reply": "2023-05-01T16:06:02.476024Z",
     "shell.execute_reply.started": "2023-05-01T16:06:02.475996Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert1(predicts):\n",
    "    a=[]\n",
    "    b=[]\n",
    "    for i in range(len(predicts)):\n",
    "        b=[]\n",
    "        for j in range(len(predicts[i])):\n",
    "            b.append(charl[int(predicts[i][j])])\n",
    "        a.append(b)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607b5167",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-01T16:06:02.478198Z",
     "iopub.status.idle": "2023-05-01T16:06:02.479080Z",
     "shell.execute_reply": "2023-05-01T16:06:02.478847Z",
     "shell.execute_reply.started": "2023-05-01T16:06:02.478812Z"
    }
   },
   "outputs": [],
   "source": [
    "def trim(yt):\n",
    "    yt1=[]\n",
    "    for i in range(len(yt)):\n",
    "        k=0\n",
    "        for j in range(26,0,-1):\n",
    "            if yt[i][j]!=0:\n",
    "                yt1.append(yt[i][0:j+1])\n",
    "                k=1\n",
    "                break\n",
    "        if k==0:\n",
    "            yt1.append([0])\n",
    "    return yt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbe7968",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-01T16:06:02.480599Z",
     "iopub.status.idle": "2023-05-01T16:06:02.481596Z",
     "shell.execute_reply": "2023-05-01T16:06:02.481355Z",
     "shell.execute_reply.started": "2023-05-01T16:06:02.481326Z"
    }
   },
   "outputs": [],
   "source": [
    "yt1=trim(yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6094f6f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-01T16:06:02.483334Z",
     "iopub.status.idle": "2023-05-01T16:06:02.484544Z",
     "shell.execute_reply": "2023-05-01T16:06:02.484268Z",
     "shell.execute_reply.started": "2023-05-01T16:06:02.484240Z"
    }
   },
   "outputs": [],
   "source": [
    "len(yt1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db91941",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-01T16:06:02.486040Z",
     "iopub.status.idle": "2023-05-01T16:06:02.486921Z",
     "shell.execute_reply": "2023-05-01T16:06:02.486664Z",
     "shell.execute_reply.started": "2023-05-01T16:06:02.486635Z"
    }
   },
   "outputs": [],
   "source": [
    "len(yt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6684cb8",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-01T16:06:02.488698Z",
     "iopub.status.idle": "2023-05-01T16:06:02.489539Z",
     "shell.execute_reply": "2023-05-01T16:06:02.489301Z",
     "shell.execute_reply.started": "2023-05-01T16:06:02.489272Z"
    }
   },
   "outputs": [],
   "source": [
    "yt1[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced4d5f3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-01T16:06:02.491201Z",
     "iopub.status.idle": "2023-05-01T16:06:02.492072Z",
     "shell.execute_reply": "2023-05-01T16:06:02.491828Z",
     "shell.execute_reply.started": "2023-05-01T16:06:02.491801Z"
    }
   },
   "outputs": [],
   "source": [
    "predicts1=convert(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766c1a4c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-01T16:06:02.495090Z",
     "iopub.status.idle": "2023-05-01T16:06:02.495869Z",
     "shell.execute_reply": "2023-05-01T16:06:02.495641Z",
     "shell.execute_reply.started": "2023-05-01T16:06:02.495613Z"
    }
   },
   "outputs": [],
   "source": [
    "gt=convert1(yt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f392c5",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-01T16:06:02.497218Z",
     "iopub.status.idle": "2023-05-01T16:06:02.497940Z",
     "shell.execute_reply": "2023-05-01T16:06:02.497706Z",
     "shell.execute_reply.started": "2023-05-01T16:06:02.497678Z"
    }
   },
   "outputs": [],
   "source": [
    "from data import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54275408",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-01T16:06:02.499342Z",
     "iopub.status.idle": "2023-05-01T16:06:02.500063Z",
     "shell.execute_reply": "2023-05-01T16:06:02.499818Z",
     "shell.execute_reply.started": "2023-05-01T16:06:02.499792Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import string\n",
    "import unicodedata\n",
    "import editdistance\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def ocr_metrics(predicts, ground_truth, norm_accentuation=False, norm_punctuation=False):\n",
    "    \"\"\"Calculate Character Error Rate (CER), Word Error Rate (WER) and Sequence Error Rate (SER)\"\"\"\n",
    "\n",
    "    if len(predicts) == 0 or len(ground_truth) == 0:\n",
    "        return (1, 1, 1)\n",
    "\n",
    "    cer, wer, ser = [], [], []\n",
    "\n",
    "    for (pd, gt) in zip(predicts, ground_truth):\n",
    "        '''pd, gt = pd.lower(), gt.lower()\n",
    "\n",
    "        if norm_accentuation:\n",
    "            pd = unicodedata.normalize(\"NFKD\", pd).encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n",
    "            gt = unicodedata.normalize(\"NFKD\", gt).encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n",
    "\n",
    "        if norm_punctuation:\n",
    "            pd = pd.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "            gt = gt.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\t'''\n",
    "        pd_cer, gt_cer = list(pd), list(gt)\n",
    "        dist = editdistance.eval(pd_cer, gt_cer)\n",
    "        cer.append(dist / (max(len(pd_cer), len(gt_cer))))\n",
    "'''\n",
    "        pd_wer, gt_wer = pd, gt\n",
    "        dist = editdistance.eval(pd_wer, gt_wer)\n",
    "        wer.append(dist / (max(len(pd_wer), len(gt_wer))))\n",
    "        \n",
    "        pd_ser, gt_ser = [pd], [gt]\n",
    "        dist = editdistance.eval(pd_ser, gt_ser)\n",
    "        ser.append(dist / (max(len(pd_ser), len(gt_ser))))\n",
    "        '''\n",
    "    metrics = [cer, wer]\n",
    "    metrics = np.mean(metrics, axis=1)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af430ae4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-01T16:06:02.501421Z",
     "iopub.status.idle": "2023-05-01T16:06:02.502140Z",
     "shell.execute_reply": "2023-05-01T16:06:02.501904Z",
     "shell.execute_reply.started": "2023-05-01T16:06:02.501875Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate = ocr_metrics(predicts=predicts1,\n",
    "                                  ground_truth=gt,)\n",
    " \n",
    "print(\"Calculate Character Error Rate {} \".format(evaluate[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ff1a18",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-01T16:06:02.503510Z",
     "iopub.status.idle": "2023-05-01T16:06:02.504238Z",
     "shell.execute_reply": "2023-05-01T16:06:02.503985Z",
     "shell.execute_reply.started": "2023-05-01T16:06:02.503958Z"
    }
   },
   "outputs": [],
   "source": [
    "predicts1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86959f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74d9015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96cafd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbca3c81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e51de7d2",
   "metadata": {},
   "source": [
    "# charlist generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc5608f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138bd757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a98028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a879bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b6fb52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
