{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f455b219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c45f574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/images/data'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 8\n",
    "ARCH = \"puigcerver\"\n",
    "\n",
    "IMG_SIZE = (128,32, 1)\n",
    "DATA_ROOT_PATH = \"../data\"\n",
    "IMAGES_PATH = os.path.join(DATA_ROOT_PATH, \"images\", \"data\")\n",
    "IMAGES_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f915efde",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = None\n",
    "\n",
    "with open(os.path.join(DATA_ROOT_PATH, \"hindi_vocab.txt\"),encoding=\"utf-8\") as f:\n",
    "  vocab = f.readlines()\n",
    "\n",
    "idx_to_vocab = {i:value.strip() for i, value in enumerate(vocab)}\n",
    "vocab_to_idx = {value:key for key, value in idx_to_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "113fec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = None\n",
    "\n",
    "with open(os.path.join(DATA_ROOT_PATH, \"new_train.txt\"), encoding=\"utf-8\") as f:\n",
    "  train_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a4ede1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3870ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = None\n",
    "\n",
    "with open(os.path.join(DATA_ROOT_PATH, \"new_val.txt\"), encoding=\"utf-8\") as f:\n",
    "  valid_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01e965cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = None\n",
    "\n",
    "with open(os.path.join(DATA_ROOT_PATH, \"new_test.txt\"), encoding=\"utf-8\") as f:\n",
    "  test_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66f3e8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data1 = None\n",
    "\n",
    "with open(os.path.join(DATA_ROOT_PATH, \"new_te.txt\"), encoding=\"utf-8\") as f:\n",
    "  test_data1 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "763be9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "charl = None\n",
    "\n",
    "with open( \"charList.txt\", encoding=\"utf-8\") as f:\n",
    "  charl = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ea2f9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "charl=charl[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "696dbd78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'उ'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charl[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39201a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12865"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79d4bc75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69803"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e15cf82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12713"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c90152d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4000):\n",
    "    train_data.append(valid_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68de0b7",
   "metadata": {},
   "source": [
    "# train data labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063b4316",
   "metadata": {},
   "source": [
    "# train data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05b19bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 15:32:53.332291: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-09 15:32:53.867377: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-09 15:32:53.867429: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-09 15:32:53.867435: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import tensorflow as tf\n",
    "from PIL import Image, ImageOps\n",
    "from data import preproc as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c682bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##label generator\n",
    "def label_g(l):\n",
    "    label=idx_to_vocab[l]\n",
    "    z=[]\n",
    "    for j in range(27):\n",
    "        z.append(0)\n",
    "    for k in range(len(label)):\n",
    "        for r in range(len(charl)):\n",
    "            if label[k]==charl[r]:\n",
    "                z[k]=r\n",
    "    return z   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b70735d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##label generator\n",
    "def label_g1(l):\n",
    "    label=idx_to_vocab[l]\n",
    "    z=[]\n",
    "    for k in range(len(label)):\n",
    "        for r in range(len(charl)):\n",
    "            if label[k]==charl[r]:\n",
    "                z.append(r)\n",
    "    return z   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8dcde742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HindiSeg/train/10/259/1.jpg 10800\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5664503e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a87fb958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "999e70f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=np.zeros((8, 27))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c59e7bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import tensorflow as tf\n",
    "from PIL import Image, ImageOps\n",
    "from data import preproc as pp\n",
    "import cv2\n",
    "\n",
    "class DataGen(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data, img_size, batch_size, mode=\"TRAIN\"):\n",
    "        self.data = data\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        start = i * self.batch_size\n",
    "        end = (i+1) * self.batch_size\n",
    "        batch_images = np.zeros((self.batch_size, self.img_size[0], self.img_size[1], 1))\n",
    "        batch_labels = np.zeros((self.batch_size, 27))\n",
    "        #batch_labels =[[],[],[],[],[],[],[],[]]\n",
    "        \n",
    "        for ii, df_index in enumerate(range(start, end)):\n",
    "            curr_data = self.data[df_index].split()\n",
    "            curr_img_path = curr_data[0]\n",
    "            #curr_label = idx_to_vocab[int(curr_data[1])]\n",
    "            curr_label = label_g(int(curr_data[1]))\n",
    "\n",
    "            curr_img_path = \"/\".join(curr_img_path.split(\"/\")[2:])\n",
    "            curr_img_path = os.path.join(IMAGES_PATH, curr_img_path)\n",
    "\n",
    "            \n",
    "            curr_img = pp.preprocess(curr_img_path, self.img_size)\n",
    "            curr_img=cv2.adaptiveThreshold(curr_img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY, 199, 5)\n",
    "            #curr_img = tf.image.resize(curr_img, (self.img_size[0], self.img_size[1]), method=\"nearest\")\n",
    "            #curr_img = curr_img.numpy().reshape((self.img_size[0], self.img_size[1],1))\n",
    "            batch_images[ii, :, :,0] = curr_img \n",
    "            batch_labels[ii,:] = curr_label\n",
    "                    \n",
    "        if self.mode == \"TRAIN\":\n",
    "          return batch_images, batch_labels\n",
    "        else:\n",
    "          return batch_images\n",
    "        \n",
    "    def __len__(self):\n",
    "      return len(self.data) // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6191b856",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = DataGen(train_data, IMG_SIZE, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a13e945",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_datagen = DataGen(valid_data, IMG_SIZE, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f42974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = DataGen(test_data1, IMG_SIZE, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0ec0720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, False, False, False, False, False,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [False, False, False, False, False, False, False, False,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [False, False, False, False, False,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [False, False, False, False, False, False,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [False,  True, False, False, False, False, False, False, False,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [False, False, False, False, False, False, False,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [False, False, False, False, False, False, False, False,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datagen[0][1]==train_datagen[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3413c386",
   "metadata": {},
   "source": [
    "# model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47721ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "from network.layers import FullGatedConv2D, GatedConv2D, OctConv2D\n",
    "from tensorflow.keras.layers import Conv2D, Bidirectional, LSTM, GRU, Dense\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization, LeakyReLU, PReLU\n",
    "from tensorflow.keras.layers import Input, Add, Activation, Lambda, MaxPooling2D, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18ae36f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86ed9706",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(\"..\", \"output\",str(datetime.now()), ARCH)\n",
    "target_path = os.path.join(output_path, \"checkpoint_weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24be974f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2023, 5, 9, 15, 33, 0, 589487)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b515e23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 128, 32, 1)]      0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 128, 32, 16)       160       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 128, 32, 16)      64        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 128, 32, 16)       0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 64, 16, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 64, 16, 32)        4640      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 64, 16, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 64, 16, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 32, 8, 32)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32, 8, 32)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 32, 8, 48)         13872     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 32, 8, 48)        192       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 32, 8, 48)         0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 16, 4, 48)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16, 4, 48)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 4, 64)         27712     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 16, 4, 64)        256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 16, 4, 64)         0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16, 4, 64)         0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 16, 4, 80)         46160     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 16, 4, 80)        320       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 16, 4, 80)         0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 16, 320)           0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 16, 512)          1181696   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 16, 512)          1574912   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 16, 512)          1574912   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 16, 512)          1574912   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, 16, 512)          1574912   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16, 512)           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16, 111)           56943     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,631,791\n",
      "Trainable params: 7,631,311\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from network.model import HTRModel\n",
    "\n",
    "# create and compile HTRModel\n",
    "model = HTRModel(architecture=ARCH,\n",
    "                 input_size=IMG_SIZE,\n",
    "                 vocab_size=110,\n",
    "                 beam_width=10,\n",
    "                 stop_tolerance=20,\n",
    "                 reduce_tolerance=15,\n",
    "                 reduce_factor=0.1)\n",
    "\n",
    "model.compile(learning_rate=0.001)\n",
    "model.summary(output_path, \"summary.txt\")\n",
    "\n",
    "callbacks = model.get_callbacks(logdir=output_path, checkpoint=target_path, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "996b6975",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "  16/9225 [..............................] - ETA: 23:32 - loss: 31.2954"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "model.fit(x=train_datagen,\n",
    "              epochs=EPOCHS,\n",
    "          validation_data=valid_datagen,\n",
    "              callbacks=callbacks,\n",
    "              verbose=1)\n",
    "total_time = datetime.now() - start_time\n",
    "\n",
    "print(\"total time\",total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "535e27ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "xt=test_datagen[0][0]\n",
    "yt=test_datagen[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46062c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 09:28:48.698778: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.51GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-05-05 09:28:48.698813: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.51GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-05-05 09:28:48.717611: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.91GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-05-05 09:28:48.717628: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.91GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-05-05 09:28:48.717636: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.13GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-05-05 09:28:48.717644: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.13GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-05-05 09:28:48.717986: W tensorflow/tsl/framework/bfc_allocator.cc:360] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\n",
      "2023-05-05 09:28:50.039539: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.81GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-05-05 09:28:50.039571: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.81GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-05-05 09:28:50.564677: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.16GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-05-05 09:28:50.564700: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.16GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-05-05 09:28:50.862511: W tensorflow/tsl/framework/bfc_allocator.cc:360] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step\n",
      "CTC Decode\n",
      "1/1 [==============================] - 86s 86s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shashank/anaconda3/envs/htr2/lib/python3.8/site-packages/numpy/core/fromnumeric.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  result = getattr(asarray(obj), method)(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "predicts, _ = model.predict(x=xt,\n",
    "                            ctc_decode=True,\n",
    "                            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31bfff8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7f9ef1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[56, 44, 40, 68]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e88242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(predicts):\n",
    "    a=[]\n",
    "    b=[]\n",
    "    for i in range(len(predicts)):\n",
    "        b=[]\n",
    "        for j in range(len(predicts[i][0])):\n",
    "            b.append(charl[predicts[i][0][j]])\n",
    "        a.append(b)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af1d19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert1(predicts):\n",
    "    a=[]\n",
    "    b=[]\n",
    "    for i in range(len(predicts)):\n",
    "        b=[]\n",
    "        for j in range(len(predicts[i])):\n",
    "            b.append(charl[int(predicts[i][j])])\n",
    "        a.append(b)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "754b41fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(yt):\n",
    "    yt1=[]\n",
    "    for i in range(len(yt)):\n",
    "        k=0\n",
    "        for j in range(26,0,-1):\n",
    "            if yt[i][j]!=0:\n",
    "                yt1.append(yt[i][0:j+1])\n",
    "                k=1\n",
    "                break\n",
    "        if k==0:\n",
    "            yt1.append([0])\n",
    "    return yt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "302c3f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt1=trim(yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c4c9021a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(yt1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c47b0cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(yt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0fee033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt1[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b7b28b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts1=convert(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "caf6fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt=convert1(yt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "013ec780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c3cd9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import string\n",
    "import unicodedata\n",
    "import editdistance\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def ocr_metrics(predicts, ground_truth, norm_accentuation=False, norm_punctuation=False):\n",
    "    \"\"\"Calculate Character Error Rate (CER), Word Error Rate (WER) and Sequence Error Rate (SER)\"\"\"\n",
    "\n",
    "    if len(predicts) == 0 or len(ground_truth) == 0:\n",
    "        return (1, 1, 1)\n",
    "\n",
    "    cer, wer, ser = [], [], []\n",
    "\n",
    "    for (pd, gt) in zip(predicts, ground_truth):\n",
    "        '''pd, gt = pd.lower(), gt.lower()\n",
    "\n",
    "        if norm_accentuation:\n",
    "            pd = unicodedata.normalize(\"NFKD\", pd).encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n",
    "            gt = unicodedata.normalize(\"NFKD\", gt).encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n",
    "\n",
    "        if norm_punctuation:\n",
    "            pd = pd.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "            gt = gt.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\t'''\n",
    "        pd_cer, gt_cer = list(pd), list(gt)\n",
    "        dist = editdistance.eval(pd_cer, gt_cer)\n",
    "        cer.append(dist / (max(len(pd_cer), len(gt_cer))))\n",
    "        '''\n",
    "        pd_wer, gt_wer = pd, gt\n",
    "        dist = editdistance.eval(pd_wer, gt_wer)\n",
    "        wer.append(dist / (max(len(pd_wer), len(gt_wer))))\n",
    "        \n",
    "        pd_ser, gt_ser = [pd], [gt]\n",
    "        dist = editdistance.eval(pd_ser, gt_ser)\n",
    "        ser.append(dist / (max(len(pd_ser), len(gt_ser))))\n",
    "        '''\n",
    "    metrics = [cer]\n",
    "    metrics = np.mean(metrics, axis=1)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e47a37a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file5=open('florcer1000.txt','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "33e23217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate Character Error Rate 0.1880140848857025 \n"
     ]
    }
   ],
   "source": [
    "evaluate = ocr_metrics(predicts=predicts1,\n",
    "                                  ground_truth=gt,)\n",
    " \n",
    "print(\"Calculate Character Error Rate {} \".format(evaluate[0],))\n",
    "file5.write(str(evaluate[0]))\n",
    "file5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "95e140a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ग', 'ो', 'र', '-', 'च', 'ु', 'ा', 'त', 'ी']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d38494d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7e688e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_path = os.path.join(\"..\", \"predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c4f205",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447278dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6cb76767",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_t_p=os.path.join(\"..\", \"predict\",ARCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "27dbf28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../predict/flor'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_t_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "814830eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1=open('flor1predict_ground','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4580f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(predicts1)):\n",
    "    for j in range(len(gt[i])):\n",
    "        file1.write(gt[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9c85d0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(predicts1)):\n",
    "    for j in range(len(gt[i])):\n",
    "        file1.write(gt[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "369773fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(predicts1)):\n",
    "    for j in range(len(gt[i])):\n",
    "        file1.write(gt[i][j])\n",
    "    file1.write('\\n')\n",
    "    for j in range(len(predicts1[i])):\n",
    "        file1.write(predicts1[i][j])\n",
    "    file1.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5fe90a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f5886a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "file2=open('flor1ground_t','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c1b7ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(gt)):\n",
    "    for j in range(len(gt[i])):\n",
    "        file2.write(gt[i][j])\n",
    "    file2.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2353a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1852867d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2ab536e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file3=open('flor1predict1','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9af38090",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(predicts1)):\n",
    "    for j in range(len(predicts1[i])):\n",
    "        file3.write(predicts1[i][j])\n",
    "    file3.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b83b1742",
   "metadata": {},
   "outputs": [],
   "source": [
    "file3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a126e5a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe5b91d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "01a95953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "505820aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def alg4(word1,word2):\n",
    "  M=[[float('inf')]*(len(word2)+1) for i in range(len(word1)+1)]\n",
    "          \n",
    "  #filling last row\n",
    "  for i in range(len(word2)+1):\n",
    "    M[len(word1)][i]=len(word2)-i\n",
    "          \n",
    "  #filling last column\n",
    "  for j in range(len(word1)+1):\n",
    "    M[j][len(word2)]=len(word1)-j\n",
    "              \n",
    "  #filling bottom to up manner\n",
    "          \n",
    "  for i in range(len(word1)-1,-1,-1):\n",
    "    for j in range(len(word2)-1,-1,-1):\n",
    "      if word1[i]==word2[j]:\n",
    "        M[i][j]=M[i+1][j+1]\n",
    "      else:\n",
    "        M[i][j]=1+min(M[i+1][j],M[i][j+1],M[i+1][j+1])\n",
    "\n",
    "  x,y=0,0\n",
    "  #print(x,y)\n",
    "  count=0\n",
    "  while x<len(M)-1 and y<len(M[0])-1:\n",
    "    current=M[x][y]\n",
    "    dia=M[x+1][y+1]\n",
    "    right=M[x][y+1]\n",
    "    bottom=M[x+1][y]\n",
    "    if dia<=right and dia<=bottom and dia<=current:\n",
    "      if dia==current-1:\n",
    "        print(\"Substitution-->\",word1[x],\"replaced by\",word2[y])\n",
    "        #array[string.printable[:95].find(word1[x]),string.printable[:95].find(word2[y])]=array[string.printable[:95].find(word1[x]),string.printable[:95].find(word2[y])]+1\n",
    "        sub[dict1[word1[x]]]=sub[dict1[word1[x]]]+1\n",
    "        subs[dict1[word2[x]]][dict1[word1[x]]]=subs[dict1[word2[x]]][dict1[word1[x]]]+1\n",
    "        count=count+1\n",
    "        x=x+1\n",
    "        y=y+1\n",
    "      else:\n",
    "        print(\"No operation-->\",word1[x])\n",
    "        x=x+1\n",
    "        y=y+1\n",
    "      \n",
    "    elif right<=bottom and right<=current:\n",
    "      print(\"Insertion\",word2[y])\n",
    "      ins[dict1[word2[y]]]=ins[dict1[word2[y]]]+1\n",
    "\n",
    "      count=count+1\n",
    "      y=y+1\n",
    "    else:\n",
    "      print(\"Deletion\",word1[x])\n",
    "      delete[dict1[word1[x]]]=delete[dict1[word1[x]]]+1\n",
    "      x=x+1\n",
    "      count=count+1\n",
    "  print(\"total operations\",count)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d1150ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete=[]\n",
    "ins=[]\n",
    "sub=[]\n",
    "\n",
    "for i in range(len(charl)+1):\n",
    "    delete.append(0)\n",
    "    ins.append(0)\n",
    "    sub.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f2c4f941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No operation--> ग\n",
      "Substitution--> ो replaced by ै\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dict1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [66]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(predicts1[i])):\n\u001b[1;32m      7\u001b[0m     s2\u001b[38;5;241m=\u001b[39ms2\u001b[38;5;241m+\u001b[39mpredicts1[i][j]\n\u001b[0;32m----> 8\u001b[0m \u001b[43malg4\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms2\u001b[49m\u001b[43m,\u001b[49m\u001b[43ms1\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [64]\u001b[0m, in \u001b[0;36malg4\u001b[0;34m(word1, word2)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubstitution-->\u001b[39m\u001b[38;5;124m\"\u001b[39m,word1[x],\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplaced by\u001b[39m\u001b[38;5;124m\"\u001b[39m,word2[y])\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#array[string.printable[:95].find(word1[x]),string.printable[:95].find(word2[y])]=array[string.printable[:95].find(word1[x]),string.printable[:95].find(word2[y])]+1\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m sub[dict1[word1[x]]]\u001b[38;5;241m=\u001b[39msub[\u001b[43mdict1\u001b[49m[word1[x]]]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     34\u001b[0m subs[dict1[word2[x]]][dict1[word1[x]]]\u001b[38;5;241m=\u001b[39msubs[dict1[word2[x]]][dict1[word1[x]]]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     35\u001b[0m count\u001b[38;5;241m=\u001b[39mcount\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dict1' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(len(gt)):\n",
    "    s1=\"\"\n",
    "    for j in range(len(gt[i])):\n",
    "        s1=s1+gt[i][j]\n",
    "    s2=\"\"\n",
    "    for j in range(len(predicts1[i])):\n",
    "        s2=s2+predicts1[i][j]\n",
    "    alg4(s2,s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f66db5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0804a1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1=open('flor1delete.txt','w')\n",
    "file2=open('flor1ins.txt','w')\n",
    "file3=open('flor1sub.txt','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3172506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(charl)):\n",
    "    print(charl[i])\n",
    "    file1.write(charl[i])\n",
    "    file1.write('\\n')\n",
    "    file1.write(str(delete[i]))\n",
    "    file1.write('\\n')\n",
    "    file2.write(charl[i])\n",
    "    file2.write('\\n')\n",
    "    file2.write(str(ins[i]))\n",
    "    file2.write('\\n')\n",
    "    file3.write(charl[i])\n",
    "    file3.write('\\n')\n",
    "    file3.write(str(sub[i]))\n",
    "    file3.write('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3102f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1.close()\n",
    "file2.close()\n",
    "file3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4de5377",
   "metadata": {},
   "outputs": [],
   "source": [
    "subs=[]\n",
    "for i in range(len(charl)):\n",
    "    z=[]\n",
    "    for j in range(len(charl)):\n",
    "        z.append(0)\n",
    "    subs.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ab3756",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(charl)):\n",
    "    \n",
    "    for j in range(len(charl)):\n",
    "        if subs[i][j]!=0:\n",
    "            print(subs[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63a8dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(charl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedc3439",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1={}\n",
    "for i in range(len(charl)):\n",
    "    dict1[charl[i]]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389b777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c7d632",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1=\"\"\n",
    "for j in range(len(gt[70])):\n",
    "    s1=s1+gt[70][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525183b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2=\"\"\n",
    "for j in range(len(predicts1[70])):\n",
    "    s2=s2+predicts1[70][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8790379",
   "metadata": {},
   "outputs": [],
   "source": [
    "alg4(s1,s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95bea55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8f3915",
   "metadata": {},
   "outputs": [],
   "source": [
    "file4=open('flor1subsc.txt','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c324d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(subs)):\n",
    "    for j in range(len(subs)):\n",
    "        file4.write(str(subs[i][j]))\n",
    "        file4.write(\" \")\n",
    "    file4.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67161faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "file4.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3edf101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6964ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt[70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab4d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts1[70]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29b4459",
   "metadata": {},
   "source": [
    "# charlist generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4cc6ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e979413d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df3eaea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf895ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19835df7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
