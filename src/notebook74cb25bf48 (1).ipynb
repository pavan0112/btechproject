{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n  try:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n    logical_gpus = tf.config.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n    print(e)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:45:13.133478Z","iopub.execute_input":"2023-05-02T07:45:13.133961Z","iopub.status.idle":"2023-05-02T07:45:13.155500Z","shell.execute_reply.started":"2023-05-02T07:45:13.133915Z","shell.execute_reply":"2023-05-02T07:45:13.152991Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"2 Physical GPUs, 2 Logical GPUs\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\n\nEPOCHS = 100\nBATCH_SIZE = 8\nARCH = \"flor\"\n\nIMG_SIZE = (128,32, 1)\nDATA_ROOT_PATH = \"/kaggle/input/htr123/Pyt/data\"\nIMAGES_PATH = os.path.join(DATA_ROOT_PATH, \"images\", \"data\")\nIMAGES_PATH","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:45:13.157919Z","iopub.execute_input":"2023-05-02T07:45:13.158306Z","iopub.status.idle":"2023-05-02T07:45:13.170042Z","shell.execute_reply.started":"2023-05-02T07:45:13.158262Z","shell.execute_reply":"2023-05-02T07:45:13.168684Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'/kaggle/input/htr123/Pyt/data/images/data'"},"metadata":{}}]},{"cell_type":"code","source":"vocab = None\n\nwith open(os.path.join(DATA_ROOT_PATH, \"hindi_vocab.txt\"),encoding=\"utf-8\") as f:\n  vocab = f.readlines()\n\nidx_to_vocab = {i:value.strip() for i, value in enumerate(vocab)}\nvocab_to_idx = {value:key for key, value in idx_to_vocab.items()}","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:45:13.171949Z","iopub.execute_input":"2023-05-02T07:45:13.172760Z","iopub.status.idle":"2023-05-02T07:45:13.208317Z","shell.execute_reply.started":"2023-05-02T07:45:13.172718Z","shell.execute_reply":"2023-05-02T07:45:13.207366Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\ntrain_data = None\n\nwith open(os.path.join(DATA_ROOT_PATH, \"new_train.txt\"), encoding=\"utf-8\") as f:\n  train_data = f.readlines()","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:45:13.211173Z","iopub.execute_input":"2023-05-02T07:45:13.211871Z","iopub.status.idle":"2023-05-02T07:45:13.270345Z","shell.execute_reply.started":"2023-05-02T07:45:13.211832Z","shell.execute_reply":"2023-05-02T07:45:13.269264Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"valid_data = None\n\nwith open(os.path.join(DATA_ROOT_PATH, \"new_val.txt\"), encoding=\"utf-8\") as f:\n  valid_data = f.readlines()","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:45:13.272143Z","iopub.execute_input":"2023-05-02T07:45:13.272968Z","iopub.status.idle":"2023-05-02T07:45:13.297642Z","shell.execute_reply.started":"2023-05-02T07:45:13.272924Z","shell.execute_reply":"2023-05-02T07:45:13.296323Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\ntest_data = None\n\nwith open(os.path.join(DATA_ROOT_PATH, \"new_val.txt\"), encoding=\"utf-8\") as f:\n  test_data = f.readlines()","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:45:13.299365Z","iopub.execute_input":"2023-05-02T07:45:13.300066Z","iopub.status.idle":"2023-05-02T07:45:13.309216Z","shell.execute_reply.started":"2023-05-02T07:45:13.300026Z","shell.execute_reply":"2023-05-02T07:45:13.308063Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"charl = None\n\nwith open( \"/kaggle/input/htr123/Pyt/src/charList.txt\", encoding=\"utf-8\") as f:\n  charl = f.readlines()\ncharl = charl[0]","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:45:13.311224Z","iopub.execute_input":"2023-05-02T07:45:13.312152Z","iopub.status.idle":"2023-05-02T07:45:13.330487Z","shell.execute_reply.started":"2023-05-02T07:45:13.312100Z","shell.execute_reply":"2023-05-02T07:45:13.329420Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# train data labels","metadata":{}},{"cell_type":"markdown","source":"# train data generation","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nimport tensorflow as tf\nfrom PIL import Image, ImageOps\n","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:45:13.331993Z","iopub.execute_input":"2023-05-02T07:45:13.333038Z","iopub.status.idle":"2023-05-02T07:45:13.341509Z","shell.execute_reply.started":"2023-05-02T07:45:13.332993Z","shell.execute_reply":"2023-05-02T07:45:13.340180Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\n##label generator\ndef label_g(l):\n    label=idx_to_vocab[l]\n    z=[]\n    for j in range(27):\n        z.append(0)\n    for k in range(len(label)):\n        for r in range(len(charl)):\n            if label[k]==charl[r]:\n                z[k]=r\n    return z   ","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:45:13.346361Z","iopub.execute_input":"2023-05-02T07:45:13.346675Z","iopub.status.idle":"2023-05-02T07:45:13.353886Z","shell.execute_reply.started":"2023-05-02T07:45:13.346648Z","shell.execute_reply":"2023-05-02T07:45:13.352644Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\n##label generator\ndef label_g1(l):\n    label=idx_to_vocab[l]\n    z=[]\n    for k in range(len(label)):\n        for r in range(len(charl)):\n            if label[k]==charl[r]:\n                z.append(r)\n                \n    return z   ","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:45:13.358574Z","iopub.execute_input":"2023-05-02T07:45:13.359447Z","iopub.status.idle":"2023-05-02T07:45:13.365807Z","shell.execute_reply.started":"2023-05-02T07:45:13.359409Z","shell.execute_reply":"2023-05-02T07:45:13.364646Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import re\nimport os\nimport cv2\nimport html\nimport string\nimport numpy as np\n\n\ndef adjust_to_see(img):\n    \"\"\"Rotate and transpose to image visualize (cv2 method or jupyter notebook)\"\"\"\n\n    (h, w) = img.shape[:2]\n    (cX, cY) = (w // 2, h // 2)\n\n    M = cv2.getRotationMatrix2D((cX, cY), -90, 1.0)\n    cos = np.abs(M[0, 0])\n    sin = np.abs(M[0, 1])\n\n    nW = int((h * sin) + (w * cos))\n    nH = int((h * cos) + (w * sin))\n\n    M[0, 2] += (nW / 2) - cX\n    M[1, 2] += (nH / 2) - cY\n\n    img = cv2.warpAffine(img, M, (nW + 1, nH + 1))\n    img = cv2.warpAffine(img.transpose(), M, (nW, nH))\n\n    return img\n\n\ndef augmentation(imgs,\n                 rotation_range=0,\n                 scale_range=0,\n                 height_shift_range=0,\n                 width_shift_range=0,\n                 dilate_range=1,\n                 erode_range=1):\n    \"\"\"Apply variations to a list of images (rotate, width and height shift, scale, erode, dilate)\"\"\"\n\n    imgs = imgs.astype(np.float32)\n    _, h, w = imgs.shape\n\n    dilate_kernel = np.ones((int(np.random.uniform(1, dilate_range)),), np.uint8)\n    erode_kernel = np.ones((int(np.random.uniform(1, erode_range)),), np.uint8)\n    height_shift = np.random.uniform(-height_shift_range, height_shift_range)\n    rotation = np.random.uniform(-rotation_range, rotation_range)\n    scale = np.random.uniform(1 - scale_range, 1)\n    width_shift = np.random.uniform(-width_shift_range, width_shift_range)\n\n    trans_map = np.float32([[1, 0, width_shift * w], [0, 1, height_shift * h]])\n    rot_map = cv2.getRotationMatrix2D((w // 2, h // 2), rotation, scale)\n\n    trans_map_aff = np.r_[trans_map, [[0, 0, 1]]]\n    rot_map_aff = np.r_[rot_map, [[0, 0, 1]]]\n    affine_mat = rot_map_aff.dot(trans_map_aff)[:2, :]\n\n    for i in range(len(imgs)):\n        imgs[i] = cv2.warpAffine(imgs[i], affine_mat, (w, h), flags=cv2.INTER_NEAREST, borderValue=255)\n        imgs[i] = cv2.erode(imgs[i], erode_kernel, iterations=1)\n        imgs[i] = cv2.dilate(imgs[i], dilate_kernel, iterations=1)\n\n    return imgs\n\n\n\"\"\"\nDeepSpell based text cleaning process.\n    Tal Weiss.\n    Deep Spelling.\n    Medium: https://machinelearnings.co/deep-spelling-9ffef96a24f6#.2c9pu8nlm\n    Github: https://github.com/MajorTal/DeepSpell\n\"\"\"\n\nRE_DASH_FILTER = re.compile(r'[\\-\\˗\\֊\\‐\\‑\\‒\\–\\—\\⁻\\₋\\−\\﹣\\－]', re.UNICODE)\nRE_APOSTROPHE_FILTER = re.compile(r'&#39;|[ʼ՚＇‘’‛❛❜ߴߵ`‵´ˊˋ{}{}{}{}{}{}{}{}{}]'.format(\n    chr(768), chr(769), chr(832), chr(833), chr(2387),\n    chr(5151), chr(5152), chr(65344), chr(8242)), re.UNICODE)\nRE_RESERVED_CHAR_FILTER = re.compile(r'[¶¤«»]', re.UNICODE)\nRE_LEFT_PARENTH_FILTER = re.compile(r'[\\(\\[\\{\\⁽\\₍\\❨\\❪\\﹙\\（]', re.UNICODE)\nRE_RIGHT_PARENTH_FILTER = re.compile(r'[\\)\\]\\}\\⁾\\₎\\❩\\❫\\﹚\\）]', re.UNICODE)\nRE_BASIC_CLEANER = re.compile(r'[^\\w\\s{}]'.format(re.escape(string.punctuation)), re.UNICODE)\n\nLEFT_PUNCTUATION_FILTER = \"\"\"!%&),.:;<=>?@\\\\]^_`|}~\"\"\"\nRIGHT_PUNCTUATION_FILTER = \"\"\"\"(/<=>@[\\\\^_`{|~\"\"\"\nNORMALIZE_WHITESPACE_REGEX = re.compile(r'[^\\S\\n]+', re.UNICODE)\n\n\ndef text_standardize(text):\n    \"\"\"Organize/add spaces around punctuation marks\"\"\"\n\n    if text is None:\n        return \"\"\n\n    text = html.unescape(text).replace(\"\\\\n\", \"\").replace(\"\\\\t\", \"\")\n\n    text = RE_RESERVED_CHAR_FILTER.sub(\"\", text)\n    text = RE_DASH_FILTER.sub(\"-\", text)\n    text = RE_APOSTROPHE_FILTER.sub(\"'\", text)\n    text = RE_LEFT_PARENTH_FILTER.sub(\"(\", text)\n    text = RE_RIGHT_PARENTH_FILTER.sub(\")\", text)\n    text = RE_BASIC_CLEANER.sub(\"\", text)\n\n    text = text.lstrip(LEFT_PUNCTUATION_FILTER)\n    text = text.rstrip(RIGHT_PUNCTUATION_FILTER)\n    text = text.translate(str.maketrans({c: f\" {c} \" for c in string.punctuation}))\n    text = NORMALIZE_WHITESPACE_REGEX.sub(\" \", text.strip())\n\n    return text\n\n\ndef normalization(imgs):\n    \"\"\"Normalize list of images\"\"\"\n\n    imgs = np.asarray(imgs).astype(np.float32)\n    _, h, w = imgs.shape\n\n    for i in range(len(imgs)):\n        m, s = cv2.meanStdDev(imgs[i])\n        imgs[i] = imgs[i] - m[0][0]\n        imgs[i] = imgs[i] / s[0][0] if s[0][0] > 0 else imgs[i]\n\n    return np.expand_dims(imgs, axis=-1)\n\n\n\"\"\"\nPreprocess metodology based in:\n    H. Scheidl, S. Fiel and R. Sablatnig,\n    Word Beam Search: A Connectionist Temporal Classification Decoding Algorithm, in\n    16th International Conference on Frontiers in Handwriting Recognition, pp. 256-258, 2018.\n\"\"\"\n\n\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef preprocess(img, input_size):\n    \"\"\"Make the process with the `input_size` to the scale resize\"\"\"\n\n    def imread(path):\n        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n        u, i = np.unique(np.array(img).flatten(), return_inverse=True)\n        background = int(u[np.argmax(np.bincount(i))])\n        # plt.imshow(img)\n        return img, background\n\n    if isinstance(img, str):\n        img, bg = imread(img)\n\n    if isinstance(img, tuple):\n        image, boundbox = img\n        img, bg = imread(image)\n\n        for i in range(len(boundbox)):\n            if isinstance(boundbox[i], float):\n                total = len(img) if i < 2 else len(img[0])\n                boundbox[i] = int(total * boundbox[i])\n            else:\n                boundbox[i] = int(boundbox[i])\n\n        img = np.asarray(img[boundbox[0]:boundbox[1], boundbox[2]:boundbox[3]], dtype=np.uint8)\n\n    wt, ht = input_size[:-1]\n    h, w = np.asarray(img).shape\n    f = max((h / ht), (w / wt) )\n\n    new_size = (max(min(wt, int(w / f)), 1), max(min(ht, int(h / f)), 1))\n\n    img = cv2.resize(img, new_size, interpolation=cv2.INTER_AREA)\n\n    target = np.ones([ht, wt], dtype=np.uint8) * bg\n    target[0:new_size[1], 0:new_size[0]] = img\n    img = target\n    # plt.imshow(img)\n    return img\n","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:45:13.367484Z","iopub.execute_input":"2023-05-02T07:45:13.368193Z","iopub.status.idle":"2023-05-02T07:45:13.619714Z","shell.execute_reply.started":"2023-05-02T07:45:13.368115Z","shell.execute_reply":"2023-05-02T07:45:13.618654Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"s=np.zeros((8, 27))\ns","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:45:13.624478Z","iopub.execute_input":"2023-05-02T07:45:13.626833Z","iopub.status.idle":"2023-05-02T07:45:13.640442Z","shell.execute_reply.started":"2023-05-02T07:45:13.626793Z","shell.execute_reply":"2023-05-02T07:45:13.639145Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataGen(tf.keras.utils.Sequence):\n    def __init__(self, data, img_size, batch_size, mode=\"TRAIN\"):\n        self.data = data\n        self.img_size = img_size\n        self.batch_size = batch_size\n        self.mode = mode\n        self.datagen = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=5, width_shift_range=0.05,\n                                                                        height_shift_range=0.05, shear_range=0.05,\n                                                                        zoom_range=0.05)\n\n    def __getitem__(self, i):\n        start = i * self.batch_size\n        end = (i+1) * self.batch_size\n        batch_images = np.zeros((self.batch_size, self.img_size[0], self.img_size[1], 1))\n        batch_labels = np.zeros((self.batch_size, 27))\n\n        for ii, df_index in enumerate(range(start, end)):\n            curr_data = self.data[ii].split()\n            curr_img_path = curr_data[0]\n          \n            curr_label = label_g(int(curr_data[1]))\n\n            curr_img_path = \"/\".join(curr_img_path.split(\"/\")[2:])\n            curr_img_path = os.path.join(IMAGES_PATH, curr_img_path)\n\n            curr_img = preprocess(curr_img_path, self.img_size)\n            curr_img = cv2.adaptiveThreshold(curr_img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 199, 5)\n            curr_img = curr_img[:, :, np.newaxis]  # add new axis for channels\n\n            # Apply data augmentation\n            curr_img = self.datagen.random_transform(curr_img)\n\n            batch_images[ii, :, :, 0] = np.transpose(curr_img, (1, 0, 2))[:, :, 0]\n            batch_labels[ii,:] = curr_label\n                    \n        if self.mode == \"TRAIN\":\n          return batch_images, batch_labels\n        else:\n          return batch_images\n        \n    def __len__(self):\n      return len(self.data) // self.batch_size\n","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:45:13.646730Z","iopub.execute_input":"2023-05-02T07:45:13.647457Z","iopub.status.idle":"2023-05-02T07:45:13.661373Z","shell.execute_reply.started":"2023-05-02T07:45:13.647415Z","shell.execute_reply":"2023-05-02T07:45:13.660129Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_datagen = DataGen(train_data, IMG_SIZE,128)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:47:13.105706Z","iopub.execute_input":"2023-05-02T07:47:13.106085Z","iopub.status.idle":"2023-05-02T07:47:13.113802Z","shell.execute_reply.started":"2023-05-02T07:47:13.106041Z","shell.execute_reply":"2023-05-02T07:47:13.112633Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"test_datagen = DataGen(test_data, IMG_SIZE, 1000\n                      )","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:45:13.676517Z","iopub.execute_input":"2023-05-02T07:45:13.677279Z","iopub.status.idle":"2023-05-02T07:45:13.685083Z","shell.execute_reply.started":"2023-05-02T07:45:13.677239Z","shell.execute_reply":"2023-05-02T07:45:13.684033Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"validation_datagen = DataGen(valid_data, IMG_SIZE, 128)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:47:17.327534Z","iopub.execute_input":"2023-05-02T07:47:17.327807Z","iopub.status.idle":"2023-05-02T07:47:17.332783Z","shell.execute_reply.started":"2023-05-02T07:47:17.327774Z","shell.execute_reply":"2023-05-02T07:47:17.331779Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# model\n","metadata":{}},{"cell_type":"code","source":"\"\"\"\nGated implementations\n    GatedConv2D: Introduce a Conv2D layer (same number of filters) to multiply with its sigmoid activation.\n    FullGatedConv2D: Introduce a Conv2D to extract features (linear and sigmoid), making a full gated process.\n                     This process will double number of filters to make one convolutional process.\n\"\"\"\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Layer, Conv2D, Multiply, Activation\n\n\"\"\"\nTensorflow Keras layer implementation of the gated convolution.\n    Args:\n        kwargs: Conv2D keyword arguments.\n    Reference:\n        T. Bluche, R. Messina,\n        Gated convolutional recurrent neural networks for multilingual handwriting recognition.\n        14th IAPR International Conference on Document Analysis andRecognition (ICDAR),\n        p. 646–651, 11 2017.\n\"\"\"\n\n\nclass GatedConv2D(Conv2D):\n    \"\"\"Gated Convolutional Class\"\"\"\n\n    def __init__(self, **kwargs):\n        super(GatedConv2D, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        \"\"\"Apply gated convolution\"\"\"\n\n        output = super(GatedConv2D, self).call(inputs)\n        linear = Activation(\"linear\")(inputs)\n        sigmoid = Activation(\"sigmoid\")(output)\n\n        return Multiply()([linear, sigmoid])\n\n    def get_config(self):\n        \"\"\"Return the config of the layer\"\"\"\n\n        config = super(GatedConv2D, self).get_config()\n        return config\n\n\n\"\"\"\nTensorflow Keras layer implementation of the gated convolution.\n    Args:\n        filters (int): Number of output filters.\n        kwargs: Other Conv2D keyword arguments.\n    Reference (based):\n        Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier,\n        Language modeling with gated convolutional networks, in\n        Proc. 34th Int. Conf. Mach. Learn. (ICML), vol. 70,\n        Sydney, Australia, pp. 933–941, 2017.\n\n        A. van den Oord and N. Kalchbrenner and O. Vinyals and L. Espeholt and A. Graves and K. Kavukcuoglu\n        Conditional Image Generation with PixelCNN Decoders, 2016\n        NIPS'16 Proceedings of the 30th International Conference on Neural Information Processing Systems\n\"\"\"\n\n\nclass FullGatedConv2D(Conv2D):\n    \"\"\"Gated Convolutional Class\"\"\"\n\n    def __init__(self, filters, **kwargs):\n        super(FullGatedConv2D, self).__init__(filters=filters * 2, **kwargs)\n        self.nb_filters = filters\n\n    def call(self, inputs):\n        \"\"\"Apply gated convolution\"\"\"\n\n        output = super(FullGatedConv2D, self).call(inputs)\n        linear = Activation(\"linear\")(output[:, :, :, :self.nb_filters])\n        sigmoid = Activation(\"sigmoid\")(output[:, :, :, self.nb_filters:])\n\n        return Multiply()([linear, sigmoid])\n\n    def compute_output_shape(self, input_shape):\n        \"\"\"Compute shape of layer output\"\"\"\n\n        output_shape = super(FullGatedConv2D, self).compute_output_shape(input_shape)\n        return tuple(output_shape[:3]) + (self.nb_filters * 2,)\n\n    def get_config(self):\n        \"\"\"Return the config of the layer\"\"\"\n\n        config = super(FullGatedConv2D, self).get_config()\n        config['nb_filters'] = self.nb_filters\n        del config['filters']\n        return config\n\n\n\"\"\"\nTensorflow Keras layer implementation of the octave convolution.\n\nReference (based):\n    Yunpeng Chen, Haoqi Fan, Bing Xu, Zhicheng Yan, Yannis Kalantidis, Marcus Rohrbach, Shuicheng Yan, Jiashi Feng.\n    Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution.\n\n    OctConv-TFKeras\n    Github: https://github.com/koshian2/OctConv-TFKeras\n\"\"\"\n\n\nclass OctConv2D(Layer):\n    \"\"\"Octave Convolutional Class\"\"\"\n\n    def __init__(self,\n                 filters,\n                 alpha,\n                 kernel_size=(3,3),\n                 strides=(1,1),\n                 padding=\"same\",\n                 kernel_initializer=\"glorot_uniform\",\n                 kernel_regularizer=None,\n                 kernel_constraint=None,\n                 **kwargs):\n        assert alpha >= 0 and alpha <= 1\n        assert filters > 0 and isinstance(filters, int)\n\n        super().__init__(**kwargs)\n\n        self.alpha = alpha\n        self.filters = filters\n        # optional values\n        self.kernel_size = kernel_size\n        self.strides = strides\n        self.padding = padding\n        self.kernel_initializer = kernel_initializer\n        self.kernel_regularizer = kernel_regularizer\n        self.kernel_constraint = kernel_constraint\n\n        # --> low channels\n        self.low_channels = int(self.filters * self.alpha)\n        # --> high channels\n        self.high_channels = self.filters - self.low_channels\n\n    def build(self, input_shape):\n        assert len(input_shape) == 2\n        assert len(input_shape[0]) == 4 and len(input_shape[1]) == 4\n        # assertion for high inputs\n        assert input_shape[0][1] // 2 >= self.kernel_size[0]\n        assert input_shape[0][2] // 2 >= self.kernel_size[1]\n        # assertion for low inputs\n        assert input_shape[0][1] // input_shape[1][1] == 2\n        assert input_shape[0][2] // input_shape[1][2] == 2\n\n        assert K.image_data_format() == \"channels_last\"\n        high_in = int(input_shape[0][3])\n        low_in = int(input_shape[1][3])\n\n        # High -> Low\n        self.high_to_high_kernel = self.add_weight(name=\"high_to_high_kernel\",\n                                                   shape=(*self.kernel_size, high_in, self.high_channels),\n                                                   initializer=self.kernel_initializer,\n                                                   regularizer=self.kernel_regularizer,\n                                                   constraint=self.kernel_constraint)\n        # High -> Low\n        self.high_to_low_kernel = self.add_weight(name=\"high_to_low_kernel\",\n                                                  shape=(*self.kernel_size, high_in, self.low_channels),\n                                                  initializer=self.kernel_initializer,\n                                                  regularizer=self.kernel_regularizer,\n                                                  constraint=self.kernel_constraint)\n\n        # Low -> High\n        self.low_to_high_kernel = self.add_weight(name=\"low_to_high_kernel\",\n                                                  shape=(*self.kernel_size, low_in, self.high_channels),\n                                                  initializer=self.kernel_initializer,\n                                                  regularizer=self.kernel_regularizer,\n                                                  constraint=self.kernel_constraint)\n        # Low -> Low\n        self.low_to_low_kernel = self.add_weight(name=\"low_to_low_kernel\",\n                                                 shape=(*self.kernel_size, low_in, self.low_channels),\n                                                 initializer=self.kernel_initializer,\n                                                 regularizer=self.kernel_regularizer,\n                                                 constraint=self.kernel_constraint)\n        super().build(input_shape)\n\n    def call(self, inputs):\n        # Input=[x^H, x^L]\n        assert len(inputs) == 2\n        high_input, low_input = inputs\n        # High -> High conv\n        high_to_high = K.conv2d(high_input, self.high_to_high_kernel,\n                                strides=self.strides, padding=self.padding,\n                                data_format=\"channels_last\")\n        # High -> low conv\n        high_to_low = K.pool2d(high_input, (2, 2), strides=(2, 2), pool_mode=\"avg\")\n        high_to_low = K.conv2d(high_to_low, self.high_to_low_kernel, strides=self.strides,\n                               padding=self.padding, data_format=\"channels_last\")\n\n        # Low -> high conv\n        low_to_high = K.conv2d(low_input, self.low_to_high_kernel,\n                               strides=self.strides, padding=self.padding,\n                               data_format=\"channels_last\")\n        low_to_high = K.repeat_elements(low_to_high, 2, axis=1)\n        low_to_high = K.repeat_elements(low_to_high, 2, axis=2)\n\n        # Low -> low conv\n        low_to_low = K.conv2d(low_input, self.low_to_low_kernel,\n                              strides=self.strides, padding=self.padding,\n                              data_format=\"channels_last\")\n\n        # cross add\n        high_add = high_to_high + low_to_high\n        low_add = low_to_low + high_to_low\n\n        return [high_add, low_add]\n\n    def compute_output_shape(self, input_shapes):\n        high_in_shape, low_in_shape = input_shapes\n        high_out_shape = (*high_in_shape[:3], self.high_channels)\n        low_out_shape = (*low_in_shape[:3], self.low_channels)\n        return [high_out_shape, low_out_shape]\n\n    def get_config(self):\n        base_config = super().get_config()\n        out_config = {\n            **base_config,\n            \"filters\": self.filters,\n            \"alpha\": self.alpha,\n            \"filters\": self.filters,\n            \"kernel_size\": self.kernel_size,\n            \"strides\": self.strides,\n            \"padding\": self.padding,\n            \"kernel_initializer\": self.kernel_initializer,\n            \"kernel_regularizer\": self.kernel_regularizer,\n            \"kernel_constraint\": self.kernel_constraint,\n        }\n        return out_config\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-05-02T07:45:13.696442Z","iopub.execute_input":"2023-05-02T07:45:13.697205Z","iopub.status.idle":"2023-05-02T07:45:13.729065Z","shell.execute_reply.started":"2023-05-02T07:45:13.697165Z","shell.execute_reply":"2023-05-02T07:45:13.728008Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.constraints import MaxNorm\n# \n# from network.layers import FullGatedConv2D, GatedConv2D, OctConv2D\nfrom tensorflow.keras.layers import Conv2D, Bidirectional, LSTM, GRU, Dense\nfrom tensorflow.keras.layers import Dropout, BatchNormalization, LeakyReLU, PReLU\nfrom tensorflow.keras.layers import Input, Add, Activation, Lambda, MaxPooling2D, Reshape","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:45:13.730659Z","iopub.execute_input":"2023-05-02T07:45:13.731075Z","iopub.status.idle":"2023-05-02T07:45:13.740701Z","shell.execute_reply.started":"2023-05-02T07:45:13.731019Z","shell.execute_reply":"2023-05-02T07:45:13.739540Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"!pip install kaldiio\n","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:45:13.742392Z","iopub.execute_input":"2023-05-02T07:45:13.742794Z","iopub.status.idle":"2023-05-02T07:45:24.973888Z","shell.execute_reply.started":"2023-05-02T07:45:13.742756Z","shell.execute_reply":"2023-05-02T07:45:24.972614Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Collecting kaldiio\n  Downloading kaldiio-2.18.0-py3-none-any.whl (28 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from kaldiio) (1.21.6)\nInstalling collected packages: kaldiio\nSuccessfully installed kaldiio-2.18.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"\nLanguage Model class.\nCreate and read the corpus with the language model file.\n\"\"\"\n\nimport os\nimport re\nimport string\n\nfrom kaldiio import WriteHelper\n\n\nclass LanguageModel():\n\n    def __init__(self, output, N=3):\n        self.output_path = os.path.join(output, \"language\")\n        self.N = N\n\n    def generate_kaldi_assets(self, dtgen, predicts):\n        # get data and ground truth lists\n        ctc_TK, space_TK, ground_truth = \"<ctc>\", \"<space>\", []\n\n        for pt in ['train', 'valid', 'test']:\n            for x in dtgen.dataset[pt]['gt']:\n                ground_truth.append([space_TK if y == \" \" else y for y in list(f\" {x} \")])\n\n        # define dataset size and default tokens\n        ds_size = dtgen.size['train'] + dtgen.size['valid'] + dtgen.size['test']\n\n        # get chars list and save with the ctc and space tokens\n        chars = list(dtgen.tokenizer.chars) + [ctc_TK]\n        chars[chars.index(\" \")] = space_TK\n\n        kaldi_path = os.path.join(self.output_path, \"kaldi\")\n        os.makedirs(kaldi_path, exist_ok=True)\n\n        with open(os.path.join(kaldi_path, \"chars.lst\"), \"w\") as lg:\n            lg.write(\"\\n\".join(chars))\n\n        ark_file_name = os.path.join(kaldi_path, \"conf_mats.ark\")\n        scp_file_name = os.path.join(kaldi_path, \"conf_mats.scp\")\n\n        # save ark and scp file (laia output/kaldi input format)\n        with WriteHelper(f\"ark,scp:{ark_file_name},{scp_file_name}\") as writer:\n            for i, item in enumerate(predicts):\n                writer(str(i + ds_size), item)\n\n        # save ground_truth.lst file with sparse sentences\n        with open(os.path.join(kaldi_path, \"ground_truth.lst\"), \"w\") as lg:\n            for i, item in enumerate(ground_truth):\n                lg.write(f\"{i} {' '.join(item)}\\n\")\n\n        # save indexes of the train/valid and test partitions\n        with open(os.path.join(kaldi_path, \"ID_train.lst\"), \"w\") as lg:\n            range_index = [str(i) for i in range(0, ds_size - dtgen.size['test'])]\n            lg.write(\"\\n\".join(range_index))\n\n        with open(os.path.join(kaldi_path, \"ID_test.lst\"), \"w\") as lg:\n            range_index = [str(i) for i in range(ds_size - dtgen.size['test'], ds_size)]\n            lg.write(\"\\n\".join(range_index))\n\n    def kaldi(self, predict=True):\n        \"\"\"\n        Kaldi Speech Recognition Toolkit with SRI Language Modeling Toolkit.\n        ** Important Note **\n        You'll need to do all by yourself:\n        1. Compile Kaldi with SRILM and OpenBLAS.\n        2. Create and add kaldi folder in the project `lib` folder (``src/lib/kaldi/``)\n        3. Generate files (search `--kaldi_assets` in https://github.com/arthurflor23/handwritten-text-recognition):\n            a. `chars.lst`\n            b. `conf_mats.ark`\n            c. `ground_truth.lst`\n            d. `ID_test.lst`\n            e. `ID_train.lst`\n        4. Add files (item 3) in the project `output` folder: ``output/<DATASET>/kaldi/``\n        More information (maybe help) in ``src/lib/kaldi-decode-script.sh`` comments.\n        References:\n            D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann,\n            P. Motlicek, Y. Qian, P. Schwarz, J. Silovsky, G. Stem- mer and K. Vesely.\n            The Kaldi speech recognition toolkit, 2011.\n            Workshop on Automatic Speech Recognition and Understanding.\n            URL: http://github.com/kaldi-asr/kaldi\n            Andreas Stolcke.\n            SRILM - An Extensible Language Modeling Toolkit, 2002.\n            Proceedings of the 7th International Conference on Spoken Language Processing (ICSLP).\n            URL: http://www.speech.sri.com/projects/srilm/\n        \"\"\"\n\n        option = \"TEST\" if predict else \"TRAIN\"\n        output = os.path.join(self.output_path, \"kaldi\")\n\n        if os.system(f\"./language/kaldi-decode-script.sh {output} {option} {self.N}\") != 0:\n            print(\"\\n##################\\n\")\n            print(\"Kaldi script error.\")\n            print(\"\\n##################\\n\")\n\n        if predict:\n            predicts = open(os.path.join(output, \"data\", \"predicts_t\")).read().splitlines()\n\n            for i, line in enumerate(predicts):\n                tokens = line.split()\n                predicts[i] = \"\".join(tokens[1:]).replace(\"<space>\", \" \").strip()\n\n            return predicts\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-05-02T07:45:24.980555Z","iopub.execute_input":"2023-05-02T07:45:24.981262Z","iopub.status.idle":"2023-05-02T07:45:25.357091Z","shell.execute_reply.started":"2023-05-02T07:45:24.981225Z","shell.execute_reply":"2023-05-02T07:45:25.355936Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"\"\"\"Handwritten Text Recognition Neural Network\"\"\"\n\nimport os\nimport logging\n\ntry:\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\"\n    logging.disable(logging.WARNING)\nexcept AttributeError:\n    pass\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom contextlib import redirect_stdout\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import Model\n\nfrom tensorflow.keras.callbacks import CSVLogger, TensorBoard, ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.constraints import MaxNorm\n\n# from network.layers import FullGatedConv2D, GatedConv2D, OctConv2D\nfrom tensorflow.keras.layers import Conv2D, Bidirectional, LSTM, GRU, Dense\nfrom tensorflow.keras.layers import Dropout, BatchNormalization, LeakyReLU, PReLU\nfrom tensorflow.keras.layers import Input, Add, Activation, Lambda, MaxPooling2D, Reshape\n\n\n\"\"\"\nHTRModel Class based on:\n    Y. Soullard, C. Ruffino and T. Paquet,\n    CTCModel: A Connectionnist Temporal Classification implementation for Keras.\n    ee: https://arxiv.org/abs/1901.07957, 2019.\n    github: https://github.com/ysoullard/HTRModel\n\n\nThe HTRModel class use Tensorflow 2 Keras module for the use of the\nConnectionist Temporal Classification (CTC) with the Hadwritten Text Recognition (HTR).\n\nIn a Tensorflow Keras Model, x is the input features and y the labels.\n\"\"\"\n\n\nclass HTRModel:\n\n    def __init__(self,\n                 architecture,\n                 input_size,\n                 vocab_size,\n                 greedy=False,\n                 beam_width=10,\n                 top_paths=1,\n                 stop_tolerance=20,\n                 reduce_tolerance=15,\n                 reduce_factor=0.1,\n                 reduce_cooldown=0):\n        \"\"\"\n        Initialization of a HTR Model.\n\n        :param\n            architecture: option of the architecture model to build and compile\n            greedy, beam_width, top_paths: Parameters of the CTC decoding\n            (see ctc decoding tensorflow for more details)\n        \"\"\"\n\n        self.architecture = globals()[architecture]\n        self.input_size = input_size\n        self.vocab_size = vocab_size\n\n        self.model = None\n        self.greedy = greedy\n        self.beam_width = beam_width\n        self.top_paths = max(1, top_paths)\n\n        self.stop_tolerance = stop_tolerance\n        self.reduce_tolerance = reduce_tolerance\n        self.reduce_factor = reduce_factor\n        self.reduce_cooldown = reduce_cooldown\n\n    def summary(self, output=None, target=None):\n        \"\"\"Show/Save model structure (summary)\"\"\"\n\n        self.model.summary()\n\n        if target is not None:\n            os.makedirs(output, exist_ok=True)\n\n            with open(os.path.join(output, target), \"w\") as f:\n                with redirect_stdout(f):\n                    self.model.summary()\n\n    def load_checkpoint(self, target):\n        \"\"\" Load a model with checkpoint file\"\"\"\n\n        if os.path.isfile(target):\n            if self.model is None:\n                self.compile()\n\n            self.model.load_weights(target)\n\n    def get_callbacks(self, logdir, checkpoint, monitor=\"val_loss\", verbose=0):\n        \"\"\"Setup the list of callbacks for the model\"\"\"\n\n        callbacks = [\n            CSVLogger(\n                filename=os.path.join(logdir, \"epochs.log\"),\n                separator=\";\",\n                append=True),\n            TensorBoard(\n                log_dir=logdir,\n                histogram_freq=10,\n                profile_batch=0,\n                write_graph=True,\n                write_images=False,\n                update_freq=\"epoch\"),\n            ModelCheckpoint(\n                filepath=checkpoint,\n                monitor=monitor,\n                save_best_only=True,\n                save_weights_only=True,\n                verbose=verbose),\n            EarlyStopping(\n                monitor=monitor,\n                min_delta=1e-8,\n                patience=self.stop_tolerance,\n                restore_best_weights=True,\n                verbose=verbose),\n            ReduceLROnPlateau(\n                monitor=monitor,\n                min_delta=1e-8,\n                factor=self.reduce_factor,\n                patience=self.reduce_tolerance,\n                cooldown=self.reduce_cooldown,\n                verbose=verbose)\n        ]\n\n        return callbacks\n\n    def compile(self, learning_rate=None, initial_step=0):\n        \"\"\"\n        Configures the HTR Model for training/predict.\n\n        :param optimizer: optimizer for training\n        \"\"\"\n\n        # define inputs, outputs and optimizer of the chosen architecture\n        inputs, outputs = self.architecture(self.input_size, self.vocab_size + 1)\n\n        if learning_rate is None:\n            learning_rate = CustomSchedule(d_model=self.vocab_size + 1, initial_step=initial_step)\n            self.learning_schedule = True\n        else:\n            self.learning_schedule = False\n\n        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n\n        # create and compile\n        self.model = Model(inputs=inputs, outputs=outputs)\n        self.model.compile(optimizer=optimizer, loss=self.ctc_loss_lambda_func)\n\n    def fit(self,\n            x=None,\n            y=None,\n            batch_size=None,\n            epochs=1,\n            verbose=1,\n            callbacks=None,\n            validation_split=0.0,\n            validation_data=None,\n            shuffle=True,\n            class_weight=None,\n            sample_weight=None,\n            initial_epoch=0,\n            steps_per_epoch=None,\n            validation_steps=None,\n            validation_freq=1,\n            max_queue_size=10,\n            workers=1,\n            use_multiprocessing=False,\n            **kwargs):\n        \"\"\"\n        Model training on data yielded (fit function has support to generator).\n        A fit() abstration function of TensorFlow 2.\n\n        Provide x parameter of the form: yielding (x, y, sample_weight).\n\n        :param: See tensorflow.keras.Model.fit()\n        :return: A history object\n        \"\"\"\n\n        # remove ReduceLROnPlateau (if exist) when use schedule learning rate\n        if callbacks and self.learning_schedule:\n            callbacks = [x for x in callbacks if not isinstance(x, ReduceLROnPlateau)]\n\n        out = self.model.fit(x=x, y=y, batch_size=batch_size, epochs=epochs, verbose=verbose,\n                             callbacks=callbacks, validation_split=validation_split,\n                             validation_data=validation_data, shuffle=shuffle,\n                             class_weight=class_weight, sample_weight=sample_weight,\n                             initial_epoch=initial_epoch, steps_per_epoch=steps_per_epoch,\n                             validation_steps=validation_steps, validation_freq=validation_freq,\n                             max_queue_size=max_queue_size, workers=workers,\n                             use_multiprocessing=use_multiprocessing, **kwargs)\n        return out\n\n    def predict(self,\n                x,\n                batch_size=None,\n                verbose=0,\n                steps=1,\n                callbacks=None,\n                max_queue_size=10,\n                workers=1,\n                use_multiprocessing=False,\n                ctc_decode=True):\n        \"\"\"\n        Model predicting on data yielded (predict function has support to generator).\n        A predict() abstration function of TensorFlow 2.\n\n        Provide x parameter of the form: yielding [x].\n\n        :param: See tensorflow.keras.Model.predict()\n        :return: raw data on `ctc_decode=False` or CTC decode on `ctc_decode=True` (both with probabilities)\n        \"\"\"\n\n        if verbose == 1:\n            print(\"Model Predict\")\n\n        out = self.model.predict(x=x, batch_size=batch_size, verbose=verbose, steps=steps,\n                                 callbacks=callbacks, max_queue_size=max_queue_size,\n                                 workers=workers, use_multiprocessing=use_multiprocessing)\n\n        if not ctc_decode:\n            return np.log(out.clip(min=1e-8)), []\n\n        steps_done = 0\n        if verbose == 1:\n            print(\"CTC Decode\")\n            progbar = tf.keras.utils.Progbar(target=steps)\n\n        batch_size = int(np.ceil(len(out) / steps))\n        input_length = len(max(out, key=len))\n\n        predicts, probabilities = [], []\n\n        while steps_done < steps:\n            index = steps_done * batch_size\n            until = index + batch_size\n\n            x_test = np.asarray(out[index:until])\n            x_test_len = np.asarray([input_length for _ in range(len(x_test))])\n\n            decode, log = K.ctc_decode(x_test,\n                                       x_test_len,\n                                       greedy=self.greedy,\n                                       beam_width=self.beam_width,\n                                       top_paths=self.top_paths)\n\n            probabilities.extend([np.exp(x) for x in log])\n            decode = [[[int(p) for p in x if p != -1] for x in y] for y in decode]\n            predicts.extend(np.swapaxes(decode, 0, 1))\n\n            steps_done += 1\n            if verbose == 1:\n                progbar.update(steps_done)\n\n        return (predicts, probabilities)\n\n    @staticmethod\n    def ctc_loss_lambda_func(y_true, y_pred):\n        \"\"\"Function for computing the CTC loss\"\"\"\n\n        if len(y_true.shape) > 2:\n            y_true = tf.squeeze(y_true)\n\n        # y_pred.shape = (batch_size, string_length, alphabet_size_1_hot_encoded)\n        # output of every model is softmax\n        # so sum across alphabet_size_1_hot_encoded give 1\n        #               string_length give string length\n        input_length = tf.math.reduce_sum(y_pred, axis=-1, keepdims=False)\n        input_length = tf.math.reduce_sum(input_length, axis=-1, keepdims=True)\n\n        # y_true strings are padded with 0\n        # so sum of non-zero gives number of characters in this string\n        label_length = tf.math.count_nonzero(y_true, axis=-1, keepdims=True, dtype=\"int64\")\n\n        loss = K.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n\n        # average loss across all entries in the batch\n        loss = tf.reduce_mean(loss)\n\n        return loss\n    def save_model(self, filepath):\n        tf.keras.models.save_model(self.model, filepath)\n\n\n\"\"\" \nCustom Schedule\n\nReference:\n    Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and\n    Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin.\n    \"Attention Is All You Need\", 2017\n    arXiv, URL: https://arxiv.org/abs/1706.03762\n\"\"\"\n\n\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    \"\"\"\n    Custom schedule of the learning rate with warmup_steps.\n    From original paper \"Attention is all you need\".\n    \"\"\"\n\n    def __init__(self, d_model, initial_step=0, warmup_steps=4000):\n        super(CustomSchedule, self).__init__()\n\n        self.d_model = d_model\n        self.d_model = tf.cast(self.d_model, dtype=\"float32\")\n        self.initial_step = initial_step\n        self.warmup_steps = warmup_steps\n\n    def __call__(self, step):\n        arg1 = tf.math.rsqrt(step + self.initial_step)\n        arg2 = step * (self.warmup_steps**-1.5)\n\n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n\n\n\"\"\"\nNetworks to the Handwritten Text Recognition Model\n\nReference:\n    Moysset, B. and Messina, R.:\n    Are 2D-LSTM really dead for offline text recognition?\n    In: International Journal on Document Analysis and Recognition (IJDAR)\n    Springer Science and Business Media LLC\n    URL: http://dx.doi.org/10.1007/s10032-019-00325-0\n\"\"\"\n\n\ndef bluche(input_size, d_model):\n    \"\"\"\n    Gated Convolucional Recurrent Neural Network by Bluche et al.\n\n    Reference:\n        Bluche, T., Messina, R.:\n        Gated convolutional recurrent neural networks for multilingual handwriting recognition.\n        In: Document Analysis and Recognition (ICDAR), 2017\n        14th IAPR International Conference on, vol. 1, pp. 646–651, 2017.\n        URL: https://ieeexplore.ieee.org/document/8270042\n    \"\"\"\n\n    input_data = Input(name=\"input\", shape=input_size)\n    cnn = Reshape((input_size[0] // 2, input_size[1] // 2, input_size[2] * 4))(input_data)\n\n    cnn = Conv2D(filters=8, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", activation=\"tanh\")(cnn)\n\n    cnn = Conv2D(filters=16, kernel_size=(2, 4), strides=(2, 4), padding=\"same\", activation=\"tanh\")(cnn)\n    cnn = GatedConv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(cnn)\n\n    cnn = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", activation=\"tanh\")(cnn)\n    cnn = GatedConv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(cnn)\n\n    cnn = Conv2D(filters=64, kernel_size=(2, 4), strides=(2, 4), padding=\"same\", activation=\"tanh\")(cnn)\n    cnn = GatedConv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(cnn)\n\n    cnn = Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", activation=\"tanh\")(cnn)\n    #cnn = MaxPooling2D(pool_size=(1, 4), strides=(1, 4), padding=\"valid\")(cnn)\n\n    shape = cnn.get_shape()\n    blstm = Reshape((shape[1], shape[2] * shape[3]))(cnn)\n\n    blstm = Bidirectional(LSTM(units=128, return_sequences=True))(blstm)\n    blstm = Dense(units=128, activation=\"tanh\")(blstm)\n\n    blstm = Bidirectional(LSTM(units=128, return_sequences=True))(blstm)\n    output_data = Dense(units=d_model, activation=\"softmax\")(blstm)\n\n    return (input_data, output_data)\n\n\ndef puigcerver(input_size, d_model):\n    \"\"\"\n    Convolucional Recurrent Neural Network by Puigcerver et al.\n\n    Reference:\n        Joan Puigcerver.\n        Are multidimensional recurrent layers really necessary for handwritten text recognition?\n        In: Document Analysis and Recognition (ICDAR), 2017 14th\n        IAPR International Conference on, vol. 1, pp. 67–72. IEEE (2017)\n\n        Carlos Mocholí Calvo and Enrique Vidal Ruiz.\n        Development and experimentation of a deep learning system for convolutional and recurrent neural networks\n        Escola Tècnica Superior d’Enginyeria Informàtica, Universitat Politècnica de València, 2018\n    \"\"\"\n\n    input_data = Input(name=\"input\", shape=input_size)\n\n    cnn = Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(input_data)\n    cnn = BatchNormalization()(cnn)\n    cnn = LeakyReLU(alpha=0.01)(cnn)\n    cnn = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")(cnn)\n\n    cnn = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(cnn)\n    cnn = BatchNormalization()(cnn)\n    cnn = LeakyReLU(alpha=0.01)(cnn)\n    cnn = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")(cnn)\n\n    cnn = Dropout(rate=0.2)(cnn)\n    cnn = Conv2D(filters=48, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(cnn)\n    cnn = BatchNormalization()(cnn)\n    cnn = LeakyReLU(alpha=0.01)(cnn)\n    cnn = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")(cnn)\n\n    cnn = Dropout(rate=0.2)(cnn)\n    cnn = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(cnn)\n    cnn = BatchNormalization()(cnn)\n    cnn = LeakyReLU(alpha=0.01)(cnn)\n\n    cnn = Dropout(rate=0.2)(cnn)\n    cnn = Conv2D(filters=80, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(cnn)\n    cnn = BatchNormalization()(cnn)\n    cnn = LeakyReLU(alpha=0.01)(cnn)\n\n    shape = cnn.get_shape()\n    blstm = Reshape((shape[1], shape[2] * shape[3]))(cnn)\n\n    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n\n    blstm = Dropout(rate=0.5)(blstm)\n    output_data = Dense(units=d_model, activation=\"softmax\")(blstm)\n\n    return (input_data, output_data)\n\nfrom tensorflow.keras.layers import Add\n\nfrom tensorflow.keras.regularizers import l2\n\ndef resnet_block(x, filters, kernel_size=(3, 3), kernel_regularizer=None):\n    shortcut = x\n\n    x = Conv2D(filters, kernel_size, padding=\"same\", kernel_initializer=\"he_uniform\", kernel_regularizer=kernel_regularizer)(x)\n    x = PReLU(shared_axes=[1, 2])(x)\n    x = BatchNormalization(renorm=True)(x)\n\n    x = Conv2D(filters, kernel_size, padding=\"same\", kernel_initializer=\"he_uniform\", kernel_regularizer=kernel_regularizer)(x)\n    x = BatchNormalization(renorm=True)(x)\n\n    x = Add()([shortcut, x])\n    x = PReLU(shared_axes=[1, 2])(x)\n\n    return x\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.layers import TimeDistributed\ndef flor(input_size, d_model, l2_reg = 5e-4):\n    kernel_regularizer = l2(l2_reg)\n\n    input_data = Input(name=\"input\", shape=input_size)\n\n    cnn = Conv2D(filters=16, kernel_size=(3, 3), strides=(2, 2), padding=\"same\", kernel_initializer=\"he_uniform\")(input_data)\n    cnn = PReLU(shared_axes=[1, 2])(cnn)\n    cnn = BatchNormalization(renorm=True)(cnn)\n    cnn = FullGatedConv2D(filters=16, kernel_size=(3, 3), padding=\"same\")(cnn)\n\n    cnn = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", kernel_initializer=\"he_uniform\")(cnn)\n    cnn = PReLU(shared_axes=[1, 2])(cnn)\n    cnn = BatchNormalization(renorm=True)(cnn)\n    cnn = FullGatedConv2D(filters=32, kernel_size=(3, 3), padding=\"same\")(cnn)\n\n    cnn = Conv2D(filters=40, kernel_size=(2, 4), strides=(2, 4), padding=\"same\", kernel_initializer=\"he_uniform\")(cnn)\n    cnn = PReLU(shared_axes=[1, 2])(cnn)\n    cnn = BatchNormalization(renorm=True)(cnn)\n    cnn = FullGatedConv2D(filters=40, kernel_size=(3, 3), padding=\"same\", kernel_constraint=MaxNorm(4, [0, 1, 2]))(cnn)\n    cnn = Dropout(rate=0.2, noise_shape=(None, 1, 1, 40))(cnn)\n\n\n    cnn = Conv2D(filters=48, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", kernel_initializer=\"he_uniform\")(cnn)\n    cnn = PReLU(shared_axes=[1, 2])(cnn)\n    cnn = BatchNormalization(renorm=True)(cnn)\n    cnn = FullGatedConv2D(filters=48, kernel_size=(3, 3), padding=\"same\", kernel_constraint=MaxNorm(4, [0, 1, 2]))(cnn)\n    cnn = Dropout(rate=0.2, noise_shape=(None, 1, 1, 48))(cnn)\n\n\n    cnn = Conv2D(filters=56, kernel_size=(2, 4), strides=(2, 4), padding=\"same\", kernel_initializer=\"he_uniform\")(cnn)\n    cnn = PReLU(shared_axes=[1, 2])(cnn)\n    cnn = BatchNormalization()(cnn)\n\n    cnn = FullGatedConv2D(filters=56, kernel_size=(3, 3), padding=\"same\", kernel_constraint=MaxNorm(4, [0, 1, 2]))(cnn)\n    cnn = Dropout(rate=0.2, noise_shape=(None, 1, 1, 56))(cnn)\n\n\n    cnn = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", kernel_initializer=\"he_uniform\", kernel_regularizer=kernel_regularizer)(cnn)\n    cnn = PReLU(shared_axes=[1, 2])(cnn)\n    cnn = BatchNormalization(renorm=True)(cnn)\n    cnn = resnet_block(cnn, filters=64, kernel_regularizer=kernel_regularizer)     \n\n    shape = cnn.get_shape()\n    bgru = Reshape((shape[1], shape[2] * shape[3]))(cnn)\n\n    bgru = Bidirectional(GRU(units=256, return_sequences=True, dropout=0.5))(bgru)\n    bgru = Dense(units=512)(bgru)\n\n    bgru = Bidirectional(GRU(units=256, return_sequences=True, dropout=0.5))(bgru)\n    bgru = Dense(units=512)(bgru)\n\n    bgru = Bidirectional(GRU(units=256, return_sequences=True, dropout=0.5))(bgru)\n\n    output_data = TimeDistributed(Dense(units=d_model, activation=\"softmax\"))(bgru)\n\n    return (input_data, output_data)\n\n\ndef puigcerver_octconv(input_size, d_model):\n    \"\"\"\n    Octave CNN by khinggan, architecture is same as puigcerver\n    \"\"\"\n\n    alpha = 0.25\n    input_data = Input(name=\"input\", shape=input_size)\n    high = input_data\n    low = tf.keras.layers.AveragePooling2D(2)(input_data)\n\n    high, low = OctConv2D(filters=16, alpha=alpha)([high, low])\n    high = BatchNormalization()(high)\n    low = BatchNormalization()(low)\n    high = LeakyReLU(alpha=0.01)(high)\n    low = LeakyReLU(alpha=0.01)(low)\n    high = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")(high)\n    low = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")(low)\n\n    high, low = OctConv2D(filters=32, alpha=alpha)([high, low])\n    high = BatchNormalization()(high)\n    low = BatchNormalization()(low)\n    high = LeakyReLU(alpha=0.01)(high)\n    low = LeakyReLU(alpha=0.01)(low)\n    high = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")(high)\n    low = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")(low)\n\n    high = Dropout(rate=0.2)(high)\n    low = Dropout(rate=0.2)(low)\n    high = Conv2D(filters=48, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(high)\n    low = Conv2D(filters=48, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(low)\n    high = BatchNormalization()(high)\n    low = BatchNormalization()(low)\n    high = LeakyReLU(alpha=0.01)(high)\n    low = LeakyReLU(alpha=0.01)(low)\n    high = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")(high)\n    low = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")(low)\n\n    high = Dropout(rate=0.2)(high)\n    low = Dropout(rate=0.2)(low)\n    high = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(high)\n    low = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(low)\n    high = BatchNormalization()(high)\n    low = BatchNormalization()(low)\n    high = LeakyReLU(alpha=0.01)(high)\n    low = LeakyReLU(alpha=0.01)(low)\n\n    high = Dropout(rate=0.2)(high)\n    low = Dropout(rate=0.2)(low)\n    high = Conv2D(filters=80, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(high)\n    low = Conv2D(filters=80, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(low)\n    high = BatchNormalization()(high)\n    low = BatchNormalization()(low)\n    high = LeakyReLU(alpha=0.01)(high)\n    low = LeakyReLU(alpha=0.01)(low)\n\n    x = _create_octconv_last_block([high, low], 80, alpha)\n\n    shape = x.get_shape()\n    blstm = Reshape((shape[1], shape[2] * shape[3]))(x)\n\n    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n\n    blstm = Dropout(rate=0.5)(blstm)\n    output_data = Dense(units=d_model, activation=\"softmax\")(blstm)\n\n    return (input_data, output_data)\n\n\ndef _create_octconv_last_block(inputs, ch, alpha):\n    high, low = inputs\n\n    high, low = OctConv2D(filters=ch, alpha=alpha)([high, low])\n    high = BatchNormalization()(high)\n    high = Activation(\"relu\")(high)\n\n    low = BatchNormalization()(low)\n    low = Activation(\"relu\")(low)\n\n    high_to_high = Conv2D(ch, 3, padding=\"same\")(high)\n    low_to_high = Conv2D(ch, 3, padding=\"same\")(low)\n    low_to_high = Lambda(lambda x: K.repeat_elements(K.repeat_elements(x, 2, axis=1), 2, axis=2))(low_to_high)\n\n    x = Add()([high_to_high, low_to_high])\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n\n    return x\n\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-05-02T07:45:25.359047Z","iopub.execute_input":"2023-05-02T07:45:25.359440Z","iopub.status.idle":"2023-05-02T07:45:25.454040Z","shell.execute_reply.started":"2023-05-02T07:45:25.359399Z","shell.execute_reply":"2023-05-02T07:45:25.453074Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\n# from network.model import HTRModel\n\n# Create and compile HTRModel\nmodel = HTRModel(architecture=ARCH,\n                 input_size=IMG_SIZE,\n                 vocab_size=110,\n                 beam_width=10,\n                 stop_tolerance=20,\n                 reduce_tolerance=15,\n                 reduce_factor=0.1)\n\nmodel.compile(learning_rate=0.0001)\n\n# Early stopping and learning rate reduction callbacks\nearly_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, restore_best_weights=True)\nreduce_lr_on_plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=15, verbose=1, min_lr=1e-6)\n\ncallbacks = [early_stopping, reduce_lr_on_plateau]\n\nmodel.fit(x=train_datagen,      \n                    epochs=5,\n                    verbose=1,  \n                    callbacks=callbacks)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T08:06:28.280624Z","iopub.execute_input":"2023-05-02T08:06:28.280944Z","iopub.status.idle":"2023-05-02T09:29:34.908243Z","shell.execute_reply.started":"2023-05-02T08:06:28.280907Z","shell.execute_reply":"2023-05-02T09:29:34.907092Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"2023-05-02 08:07:24.247522: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_3/dropout_9/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"545/545 [==============================] - 1051s 2s/step - loss: 21.4454 - lr: 1.0000e-04\nEpoch 2/5\n545/545 [==============================] - 983s 2s/step - loss: 15.3639 - lr: 1.0000e-04\nEpoch 3/5\n545/545 [==============================] - 980s 2s/step - loss: 9.3268 - lr: 1.0000e-04\nEpoch 4/5\n545/545 [==============================] - 987s 2s/step - loss: 4.7442 - lr: 1.0000e-04\nEpoch 5/5\n545/545 [==============================] - 982s 2s/step - loss: 2.2413 - lr: 1.0000e-04\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x737ca4194910>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ncurr_data = test_data[56].split()\ncurr_img_path = curr_data[0]\n\n\ncurr_img_path = \"/\".join(curr_img_path.split(\"/\")[2:])\ncurr_img_path = os.path.join(IMAGES_PATH, curr_img_path)\ncurr_img = preprocess(curr_img_path, IMG_SIZE)\ncurr_img.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-02T09:35:03.448714Z","iopub.execute_input":"2023-05-02T09:35:03.449095Z","iopub.status.idle":"2023-05-02T09:35:03.490634Z","shell.execute_reply.started":"2023-05-02T09:35:03.449052Z","shell.execute_reply":"2023-05-02T09:35:03.489755Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"(32, 128)"},"metadata":{}}]},{"cell_type":"code","source":"plt.imshow(curr_img)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T09:35:04.789540Z","iopub.execute_input":"2023-05-02T09:35:04.789816Z","iopub.status.idle":"2023-05-02T09:35:05.012154Z","shell.execute_reply.started":"2023-05-02T09:35:04.789782Z","shell.execute_reply":"2023-05-02T09:35:05.011197Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x737cb278af90>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"curr_img = np.transpose(curr_img, (1, 0))\ncurr_img = np.expand_dims(curr_img, axis=-1)\ncurr_img = np.expand_dims(curr_img, axis=0)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-02T09:35:06.223353Z","iopub.execute_input":"2023-05-02T09:35:06.224015Z","iopub.status.idle":"2023-05-02T09:35:06.230331Z","shell.execute_reply.started":"2023-05-02T09:35:06.223975Z","shell.execute_reply":"2023-05-02T09:35:06.229059Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"curr_img.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-02T09:35:14.249893Z","iopub.execute_input":"2023-05-02T09:35:14.250734Z","iopub.status.idle":"2023-05-02T09:35:14.258059Z","shell.execute_reply.started":"2023-05-02T09:35:14.250672Z","shell.execute_reply":"2023-05-02T09:35:14.257005Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"(1, 128, 32, 1)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.predict(curr_img)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T09:35:15.379240Z","iopub.execute_input":"2023-05-02T09:35:15.380299Z","iopub.status.idle":"2023-05-02T09:35:17.811044Z","shell.execute_reply.started":"2023-05-02T09:35:15.380252Z","shell.execute_reply":"2023-05-02T09:35:17.809957Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"([array([[48, 60,  3, 60, 23, 60, 23, 62, 23]])],\n [array([0.00018611], dtype=float32)])"},"metadata":{}}]},{"cell_type":"code","source":"print(charl[48])\nprint(charl[60])\nprint(charl[3])\nprint(charl[60])\nprint(charl[23])\nprint(charl[23])","metadata":{"execution":{"iopub.status.busy":"2023-05-02T09:36:35.882545Z","iopub.execute_input":"2023-05-02T09:36:35.883152Z","iopub.status.idle":"2023-05-02T09:36:35.889379Z","shell.execute_reply.started":"2023-05-02T09:36:35.883111Z","shell.execute_reply":"2023-05-02T09:36:35.888400Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"र\nा\nं\nा\nग\nग\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:46:01.096915Z","iopub.status.idle":"2023-05-02T07:46:01.097689Z","shell.execute_reply.started":"2023-05-02T07:46:01.097398Z","shell.execute_reply":"2023-05-02T07:46:01.097426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(curr_img)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:46:01.099079Z","iopub.status.idle":"2023-05-02T07:46:01.099809Z","shell.execute_reply.started":"2023-05-02T07:46:01.099532Z","shell.execute_reply":"2023-05-02T07:46:01.099559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(charl[57])\nprint(charl[60])\nprint(charl[3])\nprint(charl[23])\nprint(charl[21])\nprint(charl[60])\nprint(charl[3])\nprint(charl[23])","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:46:01.101195Z","iopub.status.idle":"2023-05-02T07:46:01.101940Z","shell.execute_reply.started":"2023-05-02T07:46:01.101645Z","shell.execute_reply":"2023-05-02T07:46:01.101683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xt=test_datagen[0][0]\nyt=test_datagen[0][1]","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:46:01.103276Z","iopub.status.idle":"2023-05-02T07:46:01.104018Z","shell.execute_reply.started":"2023-05-02T07:46:01.103735Z","shell.execute_reply":"2023-05-02T07:46:01.103761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicts, _ = model.predict(x=xt,\n                            ctc_decode=True,\n                            verbose=1)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:46:01.105419Z","iopub.status.idle":"2023-05-02T07:46:01.106162Z","shell.execute_reply.started":"2023-05-02T07:46:01.105902Z","shell.execute_reply":"2023-05-02T07:46:01.105929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(predicts[0])","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:46:01.107512Z","iopub.status.idle":"2023-05-02T07:46:01.108292Z","shell.execute_reply.started":"2023-05-02T07:46:01.107998Z","shell.execute_reply":"2023-05-02T07:46:01.108025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicts[1][0]","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:46:01.109747Z","iopub.status.idle":"2023-05-02T07:46:01.110500Z","shell.execute_reply.started":"2023-05-02T07:46:01.110215Z","shell.execute_reply":"2023-05-02T07:46:01.110241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert(predicts):\n    a=[]\n    b=[]\n    for i in range(len(predicts)):\n        b=[]\n        for j in range(len(predicts[i][0])):\n            b.append(charl[predicts[i][0][j]])\n        a.append(b)\n    return a","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:46:01.111986Z","iopub.status.idle":"2023-05-02T07:46:01.112719Z","shell.execute_reply.started":"2023-05-02T07:46:01.112439Z","shell.execute_reply":"2023-05-02T07:46:01.112467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert1(predicts):\n    a=[]\n    b=[]\n    for i in range(len(predicts)):\n        b=[]\n        for j in range(len(predicts[i])):\n            b.append(charl[int(predicts[i][j])])\n        a.append(b)\n    return a","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:46:01.114178Z","iopub.status.idle":"2023-05-02T07:46:01.114933Z","shell.execute_reply.started":"2023-05-02T07:46:01.114645Z","shell.execute_reply":"2023-05-02T07:46:01.114682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def trim(yt):\n    yt1=[]\n    for i in range(len(yt)):\n        k=0\n        for j in range(26,0,-1):\n            if yt[i][j]!=0:\n                yt1.append(yt[i][0:j+1])\n                k=1\n                break\n        if k==0:\n            yt1.append([0])\n    return yt1","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:46:01.116311Z","iopub.status.idle":"2023-05-02T07:46:01.117101Z","shell.execute_reply.started":"2023-05-02T07:46:01.116807Z","shell.execute_reply":"2023-05-02T07:46:01.116834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yt1=trim(yt)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:46:01.118499Z","iopub.status.idle":"2023-05-02T07:46:01.119260Z","shell.execute_reply.started":"2023-05-02T07:46:01.118984Z","shell.execute_reply":"2023-05-02T07:46:01.119011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(yt1[0])","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:46:01.120691Z","iopub.status.idle":"2023-05-02T07:46:01.121445Z","shell.execute_reply.started":"2023-05-02T07:46:01.121179Z","shell.execute_reply":"2023-05-02T07:46:01.121206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(yt[0])","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:46:01.122795Z","iopub.status.idle":"2023-05-02T07:46:01.123544Z","shell.execute_reply.started":"2023-05-02T07:46:01.123290Z","shell.execute_reply":"2023-05-02T07:46:01.123317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yt1[0][1]","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:46:01.124893Z","iopub.status.idle":"2023-05-02T07:46:01.125603Z","shell.execute_reply.started":"2023-05-02T07:46:01.125340Z","shell.execute_reply":"2023-05-02T07:46:01.125366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicts1=convert(predicts)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:46:01.127047Z","iopub.status.idle":"2023-05-02T07:46:01.127801Z","shell.execute_reply.started":"2023-05-02T07:46:01.127518Z","shell.execute_reply":"2023-05-02T07:46:01.127544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gt=convert1(yt1)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:46:01.129238Z","iopub.status.idle":"2023-05-02T07:46:01.130022Z","shell.execute_reply.started":"2023-05-02T07:46:01.129720Z","shell.execute_reply":"2023-05-02T07:46:01.129747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from data import evaluation","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:46:01.131424Z","iopub.status.idle":"2023-05-02T07:46:01.132151Z","shell.execute_reply.started":"2023-05-02T07:46:01.131893Z","shell.execute_reply":"2023-05-02T07:46:01.131919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport string\nimport unicodedata\nimport editdistance\nimport numpy as np\n\n\ndef ocr_metrics(predicts, ground_truth, norm_accentuation=False, norm_punctuation=False):\n    \"\"\"Calculate Character Error Rate (CER), Word Error Rate (WER) and Sequence Error Rate (SER)\"\"\"\n\n    if len(predicts) == 0 or len(ground_truth) == 0:\n        return (1, 1, 1)\n\n    cer, wer, ser = [], [], []\n\n    for (pd, gt) in zip(predicts, ground_truth):\n        '''pd, gt = pd.lower(), gt.lower()\n\n        if norm_accentuation:\n            pd = unicodedata.normalize(\"NFKD\", pd).encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n            gt = unicodedata.normalize(\"NFKD\", gt).encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n\n        if norm_punctuation:\n            pd = pd.translate(str.maketrans(\"\", \"\", string.punctuation))\n            gt = gt.translate(str.maketrans(\"\", \"\", string.punctuation))\n\t'''\n        pd_cer, gt_cer = list(pd), list(gt)\n        dist = editdistance.eval(pd_cer, gt_cer)\n        cer.append(dist / (max(len(pd_cer), len(gt_cer))))\n'''\n        pd_wer, gt_wer = pd, gt\n        dist = editdistance.eval(pd_wer, gt_wer)\n        wer.append(dist / (max(len(pd_wer), len(gt_wer))))\n        \n        pd_ser, gt_ser = [pd], [gt]\n        dist = editdistance.eval(pd_ser, gt_ser)\n        ser.append(dist / (max(len(pd_ser), len(gt_ser))))\n        '''\n    metrics = [cer, wer]\n    metrics = np.mean(metrics, axis=1)\n\n    return metrics","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:46:01.133556Z","iopub.status.idle":"2023-05-02T07:46:01.134347Z","shell.execute_reply.started":"2023-05-02T07:46:01.134067Z","shell.execute_reply":"2023-05-02T07:46:01.134094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate = ocr_metrics(predicts=predicts1,\n                                  ground_truth=gt,)\n \nprint(\"Calculate Character Error Rate {} \".format(evaluate[0],))","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:46:01.135736Z","iopub.status.idle":"2023-05-02T07:46:01.136497Z","shell.execute_reply.started":"2023-05-02T07:46:01.136207Z","shell.execute_reply":"2023-05-02T07:46:01.136234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicts1[0]","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:46:01.137972Z","iopub.status.idle":"2023-05-02T07:46:01.138731Z","shell.execute_reply.started":"2023-05-02T07:46:01.138444Z","shell.execute_reply":"2023-05-02T07:46:01.138471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# charlist generation\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}